{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning from Pre-Trained Models with Keras\n",
    "\n",
    "## Introduction\n",
    "\n",
    "ImageNet, an image recognition benchmark dataset*, helped trigger the modern AI explosion.  In 2012, the AlexNet architecture (a deep convolutional-neural-network) rocked the ImageNet benchmark competition, handily beating the next best entrant.  By 2014, all the leading competitors were deep learning based.  Since then, accuracy scores continued to improve, eventually surpassing human performance.\n",
    "\n",
    "In this hands-on tutorial we will build on this pioneering work to create our own neural-network architecture for image recognition.  Participants will use the elegant Keras deep learning programming interface to build and train TensorFlow models for image classification tasks on the CIFAR-10 / MNIST datasets*.  We will demonstrate the use of transfer learning* (to give our networks a head-start by building on top of existing, ImageNet pre-trained, network layers*), and explore how to improve model performance for standard deep learning pipelines.  We will use cloud-based interactive Jupyter notebooks to work through our explorations step-by-step.  Once participants have successfully trained their custom model we will show them how to submit their model's predictions to Kaggle for scoring*.\n",
    "\n",
    "This tutorial aims to prepare participants for the HPC Saudi 2020 Student AI Competition.\n",
    "\n",
    "Participants are expected to bring their own laptops and sign-up for free online cloud services (e.g., Google Colab, Kaggle).  They may also need to download free, open-source software prior to arriving for the workshop.\n",
    "\n",
    "This tutorial assumes some basic knowledge of neural networks. If you’re not already familiar with neural networks, then you can learn the basics concepts behind neural networks at [course.fast.ai](https://course.fast.ai/).\n",
    "\n",
    "* Tutorial materials are derived from:\n",
    "  * [PyTorch Tutorials](https://github.com/kaust-vislab/pytorch-tutorials) by David Pugh.\n",
    "  * [What is torch.nn really?](https://pytorch.org/tutorials/beginner/nn_tutorial.html) by Jeremy Howard, Rachel Thomas, Francisco Ingham.\n",
    "  * [Machine Learning Notebooks](https://github.com/ageron/handson-ml2) (2nd Ed.) by Aurélien Géron.\n",
    "  * *Deep Learning with Python* by François Chollet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jupyter Notebooks\n",
    "\n",
    "This is a Jupyter Notebook.  It provides a simple, cell-based, IDE for developing and exploring complex ideas via code, visualizations, and documentation.\n",
    "\n",
    "A notebook has two primary types of cells: i) `markdown` cells for textual notes and documentation, such as the one you are reading now, and ii) `code` cells, which contain snippets of code (typically *Python*, but also *bash* scripts) that can be executed.  \n",
    "\n",
    "The currently selected cell appears within a box. A green box indicates that the cell is editable.  Clicking inside a *code* cell makes it selected and editable.  Double-click inside *markdown* cells to edit.\n",
    "\n",
    "Use `Tab` for context-sensitive code-completion assistance when editing Python code in *code* cells.  For example, use code assistance after a `.` seperator to find available object members.  For help documentation, create a new *code* cell, and use commands like `dir(`*module*`)`, `help(`*topic*`)`, `?`*name*, or `??`*function* for user provided *module*, *topic*, variable *name*, or *function* name.  The magic `?` and `??` commands show documentation / source code in a separate pane.\n",
    "\n",
    "Clicking on `[Run]` or pressing `Ctrl-Enter` will execute the contents of a cell.  A *markdown* cell converts to its display version, and a *code* cell runs the code inside.  To the left of a *code* cell is a small text bracket `In [ ]:`.  If the bracket contains an asterix, e.g., `In [*]:`, that cell is currently executing.  Only one cell executes at a time (if multiple cells are *Run*, they are queued up to execute in the order they were run).  When a *code* cell finishes executing, the bracket shows an execution count in the bracket – each *code* cell execution increments the counter and provides a way to determine the order in which codes were executed – e.g., `In [7]` for the seventh cell to complete.  \n",
    "\n",
    "The output produced by a *code* cell appears at the bottom of that cell after it executes.  The output generated by a code cell includes anything printed to the output during execution (e.g., print statements, or thrown errors) and the final value generated by the cell (i.e., not the intermediate values).  The final value is 'pretty printed' by Jupyter.\n",
    "\n",
    "Typically, notebooks are written to be executed in order, from top to bottom.  Behind the scenes, however, each Notebook has a single Python state (the `kernel`), and each *code* cell that executes, modifies that state.  It is possible to modify and re-run earlier cells; however, care must be taken to also re-run any other cells that depend upon the modified one.  List the Python state global variables with the magic command `%wgets`.  The *kernel* can be restarted to a known state, and cell output cleared, if the Python state becomes too confusing to fix manually (choose `Restart & Clear Output` from the Jupyter `Kernel` menu) – this requires running each *code* cell again.\n",
    "\n",
    "Complete user documentation is available at [jupyter-notebook.readthedocs.io](https://jupyter-notebook.readthedocs.io/en/stable/notebook.html#notebook-user-interface). <br/>\n",
    "Many helpful tips and techniques from [28 Jupyter Notebook Tips, Tricks, and Shortcuts](https://www.dataquest.io/blog/jupyter-notebook-tips-tricks-shortcuts/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Kaggle Account\n",
    "\n",
    "#### 1. Register for an account\n",
    "\n",
    "In order to download Kaggle competition data you will first need to create a [Kaggle](https://www.kaggle.com/) account.\n",
    "\n",
    "#### 2. Create an API key\n",
    "\n",
    "Once you have registered for a Kaggle account you will need to create some [API credentials](https://github.com/Kaggle/kaggle-api#api-credentials) in order to be able to use the `kaggle` CLI to download data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download MNIST Data\n",
    "\n",
    "If you are using Binder to run this notebook, then the data is already downloaded and available.  Skip to the next step.\n",
    "\n",
    "If you are using Google Colab to run this notebook, then you will need to download the data before proceeding.\n",
    "\n",
    "#### Download MNIST from Kaggle\n",
    "\n",
    "Provide your Kaggle username and API key in the cell below and execute the code to download the Kaggle [Digit Recognizer: Learn computer vision with the famous MNIST data](https://www.kaggle.com/c/digit-recognizer) competition data. \n",
    "\n",
    "**Note: Before attempting to download the competition data you will need to login to your Kaggle account and accept the rules for this competition.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# NOTE: Replace YOUR_USERNAME and YOUR_API_KEY with actual credentials \n",
    "export KAGGLE_USERNAME=\"YOUR_USERNAME\"\n",
    "export KAGGLE_KEY=\"YOUR_API_KEY\"\n",
    "kaggle competitions download -c digit-recognizer -p ../datasets/mnist/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (Alternative) Download MNIST from GitHub\n",
    "\n",
    "If you are running this notebook using Google Colab, but did create a Kaggle account and API key, then  dowload the data from our GitHub repository by running the code in the following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import requests\n",
    "\n",
    "#RAW_URL = \"https://raw.githubusercontent.com/kaust-vislab/keras-tutorials/master/mnist/data/raw\"\n",
    "RAW_URL = \"https://github.com/holstgr-kaust/keras-tutorials/raw/master/datasets/mnist\"\n",
    "DEST_DIR = pathlib.Path('../datasets/mnist')\n",
    "\n",
    "def fetch_mnist_data():\n",
    "    DEST_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    for n in [\"mnist.npz\", \"kaggle/train.csv\", \"kaggle/test.csv\", \"kaggle/sample_submission.csv\"]:\n",
    "        path = DEST_DIR / n\n",
    "        with path.open(mode = 'wb') as f:\n",
    "            response = requests.get(RAW_URL + \"/\" + n)\n",
    "            f.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch_mnist_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (Alternative) Download MNIST with Keras\n",
    "\n",
    "If you are running this notebook using Google Colab, but did create a Kaggle account and API key, then dowload the data using the Keras load_data() API by running the code in the following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "mnist.load_data();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download CIFAR10 Data\n",
    "\n",
    "If you are using Binder to run this notebook, then the data is already downloaded and available.  Skip to the next step.\n",
    "\n",
    "If you are using Google Colab to run this notebook, then you will need to download the data before proceeding.\n",
    "\n",
    "#### Download CIFAR10 from Kaggle\n",
    "\n",
    "Provide your Kaggle username and API key in the cell below and execute the code to download the Kaggle [CIFAR-10 keras files](https://www.kaggle.com/guesejustin/cifar10-keras-files-cifar10load-data) competition data. \n",
    "\n",
    "**TODO:** Fix kaggle CLI download command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# NOTE: Replace YOUR_USERNAME and YOUR_API_KEY with actual credentials \n",
    "export KAGGLE_USERNAME=\"YOUR_USERNAME\"\n",
    "export KAGGLE_KEY=\"YOUR_API_KEY\"\n",
    "# https://www.kaggle.com/guesejustin/cifar10-keras-files-cifar10load-data/download\n",
    "kaggle competitions download -c guesejustin/cifar10-keras-files-cifar10load-data -p ../datasets/cifar10/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (Alternative) Download CIFAR10 from GitHub\n",
    "\n",
    "If you are running this notebook using Google Colab, but did create a Kaggle account and API key, then  dowload the data from our GitHub repository by running the code in the following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import requests\n",
    "\n",
    "#RAW_URL = \"https://raw.githubusercontent.com/kaust-vislab/keras-tutorials/master/datasets/cifar10\"\n",
    "RAW_URL = \"https://github.com/holstgr-kaust/keras-tutorials/raw/master/datasets/cifar10\"\n",
    "DEST_DIR = pathlib.Path('../datasets/cifar10')\n",
    "\n",
    "def fetch_cifar10_data():\n",
    "    DEST_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    for n in [\"cifar-10.npz\", \"cifar-10-batches-py.tar.gz\"]:\n",
    "        path = DEST_DIR / n\n",
    "        with path.open(mode = 'wb') as f:\n",
    "            response = requests.get(RAW_URL + \"/\" + n)\n",
    "            f.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch_cifar10_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "DEST_DIR='../datasets/cifar10'\n",
    "tar xvpf \"${DEST_DIR}/cifar-10-batches-py.tar.gz\" --directory=\"${DEST_DIR}\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (Alternative) Download CIFAR10 with Keras\n",
    "\n",
    "If you are running this notebook using Google Colab, but did *not* create a Kaggle account and API key, then dowload the data using the Keras load_data() API by running the code in the following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import cifar10\n",
    "cifar10.load_data();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial\n",
    "\n",
    "### Python Initialization\n",
    "\n",
    "Import the modules we will use.  `%matplotlib inline` is a magic command that makes *matplotlib* charts and plots appear was outputs in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`%matplotlib inline` enables plots to appear in cell output.\n",
    "\n",
    "`%matplotlib notebook` enables semi-interactive plots that can be enlarged, zoomed, and cropped while the plot is active.  One issue with this option is that new plots appear in the active plot widget, not in the cell where the data was produced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"executing_eagerly:\", tf.executing_eagerly())\n",
    "print(\"is_gpu_available:\", tf.test.is_gpu_available(), tf.test.gpu_device_name())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Pre-processing - MNIST\n",
    "\n",
    "The previously acquired MNIST dataset is the essential input needed to train an image classification model. Before using the dataset, there are several preprocessing steps required to load the data, and create the correctly sized training, validation, and testing arrays used as input to the network.\n",
    "\n",
    "The following data preparation steps are needed before they can become inputs to the network:\n",
    "\n",
    "* Cache the downloaded dataset (to use Keras `load_data()` functionality).\n",
    "* Load the dataset (MNIST is small, and fits in memory).\n",
    "    * Convert from textual CSV files into binary tensor arrays (https://www.tensorflow.org/tutorials/load_data/csv).\n",
    "    * Reshape from (784, 1) to (28, 28,1) to (32, 32, 3)\n",
    "* Verify the shape and type of the data, and understand it...\n",
    "* Convert label indices into categorical vectors.\n",
    "* Convert image data from integer to float values, and normalize.\n",
    "  * Verify converted input data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cache Data\n",
    "\n",
    "Make downloaded data available to Keras.  Provide dataset utility functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cache MNIST Datasets\n",
    "\n",
    "for n in [\"mnist.npz\", \"kaggle/train.csv\", \"kaggle/test.csv\"]:\n",
    "    #DATA_URL = \"https://github.com/holstgr-kaust/keras-tutorials/raw/master/datasets/mnist/%s\" % n\n",
    "    DATA_URL = \"file:///\" + str(pathlib.Path(\"../datasets/mnist/%s\" % n).absolute())\n",
    "    #data_file_path = tf.keras.utils.get_file(p + n, DATA_URL)\n",
    "    data_file_path = tf.keras.utils.get_file(n.replace('/','-mnist-'), DATA_URL)\n",
    "    print(\"cached file: %s\" % n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "find ~/.keras -name \"*mnist*\" -type f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_csv_dataset(file_path, **kwargs):\n",
    "    dataset = tf.data.experimental.make_csv_dataset(\n",
    "        file_path,\n",
    "        batch_size=5, # Artificially small to make examples easier to show.\n",
    "        label_name='label',\n",
    "        na_value=\"?\",\n",
    "        num_epochs=1,\n",
    "        ignore_errors=True, \n",
    "        **kwargs)\n",
    "    return dataset\n",
    "\n",
    "def pack(features, label):\n",
    "    return tf.stack(list(features.values()), axis=-1), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_batch(dataset):\n",
    "    for batch, label in dataset.take(1):\n",
    "        print(\"{:20s}: {} :: {}\".format('label', label, type(label)))\n",
    "        for key, value in batch.items():\n",
    "              print(\"{:20s}: {} :: {}\".format(key, value.numpy(), type(value)))\n",
    "\n",
    "def show_packed(packed_dataset):\n",
    "    for features, labels in packed_dataset.take(1):\n",
    "        print(features.numpy())\n",
    "        print()\n",
    "        print(labels.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_path = \"../datasets/mnist/kaggle/train.csv\"\n",
    "test_file_path = \"../datasets/mnist/kaggle/test.csv\"\n",
    "\n",
    "raw_train_data = get_csv_dataset(train_file_path)\n",
    "packed_train_data = raw_train_data.map(pack)\n",
    "train_data = packed_train_data.shuffle(500)\n",
    "\n",
    "# NOTE: unlabelled Kaggle test dataset?\n",
    "#raw_test_data = get_dataset(test_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Alternative) Load data via Keras API.  This loads data into a `numpy` array, and the test examples are labelled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO: complete example and convert numpy array into Dataset\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "# The data, split between train and test sets:\n",
    "(x_train_mnist, y_train_mnist), (x_test_mnist, y_test_mnist) = mnist.load_data()\n",
    "\n",
    "#(x_train_mnist, y_train_mnist, x_test_mnist, y_test_mnist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO: Modify Explore Data examples to use Dataset, move into Explore Data section**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('x_train type:', type(x_train_mnist), ',', 'y_train type:', type(y_train_mnist))\n",
    "print('x_train dtype:', x_train_mnist.dtype, ',', 'y_train dtype:', y_train_mnist.dtype)\n",
    "print('x_train shape:', x_train_mnist.shape, ',', 'y_train shape:', y_train_mnist.shape)\n",
    "print('x_test shape:', x_test_mnist.shape, ',', 'y_test shape:', y_test_mnist.shape)\n",
    "print(x_train_mnist.shape[0], 'train samples')\n",
    "print(x_test_mnist.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('x_train (min, max, mean): (%s, %s, %s)' % (x_train_mnist.min(), x_train_mnist.max(), x_train_mnist.mean()))\n",
    "print('y_train (min, max): (%s, %s)' % (y_train_mnist.min(), y_train_mnist.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Show array of random labelled images with matplotlib (re-run cell to see new examples)\n",
    "\n",
    "fig = plt.figure(figsize=(16,8))\n",
    "\n",
    "for i in range(40):\n",
    "    plt.subplot(4, 10, i + 1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    idx = int(random.uniform(0, x_train_mnist.shape[0]))\n",
    "    plt.title(y_train_mnist[idx])\n",
    "    plt.imshow(x_train_mnist[idx], cmap=plt.get_cmap('gray'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist, bins = np.histogram(y_train_mnist, bins = range(y_train_mnist.min(), y_train_mnist.max() + 2))\n",
    "\n",
    "fig = plt.figure(figsize=(12,5))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.hist(y_train_mnist, bins = range(y_train_mnist.min(), y_train_mnist.max() + 2))\n",
    "plt.xticks(range(y_train_mnist.min(), y_train_mnist.max() + 2))\n",
    "plt.title(\"y_train histogram\")\n",
    "plt.subplot(1,2,2)\n",
    "plt.hist(x_train_mnist.flat, bins = range(x_train_mnist.min(), x_train_mnist.max() + 2))\n",
    "plt.title(\"x_train histogram\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('y_train histogram counts:', hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore Data\n",
    "\n",
    "Explore data types, shape, and value ranges.  Ensure they make sense, and you understand the data well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_batch(raw_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_packed(packed_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "packed_train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIFAR10 - Dataset Processing\n",
    "\n",
    "The previously acquired CIFAR10 dataset is the essential input needed to train an image classification model. Before using the dataset, there are several preprocessing steps required to load the data, and create the correctly sized training, validation, and testing arrays used as input to the network.\n",
    "\n",
    "The following data preparation steps are needed before they can become inputs to the network:\n",
    "\n",
    "* Cache the downloaded dataset (to use Keras `load_data()` functionality).\n",
    "* Load the dataset (CIFAR10 is small, and fits into a `numpy` array).\n",
    "* Verify the shape and type of the data, and understand it...\n",
    "* Convert label indices into categorical vectors.\n",
    "* Convert image data from integer to float values, and normalize.\n",
    "  * Verify converted input data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cache Data\n",
    "\n",
    "Make downloaded data available to Keras.  Provide dataset utility functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cache CIFAR10 Datasets\n",
    "\n",
    "for n in [\"cifar-10.npz\", \"cifar-10-batches-py.tar.gz\"]:\n",
    "    #DATA_URL = \"https://github.com/holstgr-kaust/keras-tutorials/raw/master/datasets/cifar10/%s\" % n\n",
    "    DATA_URL = \"file:///\" + str(pathlib.Path(\"../datasets/cifar10/%s\" % n).absolute())\n",
    "    data_file_path = tf.keras.utils.get_file(n, DATA_URL)\n",
    "    print(\"cached file: %s\" % n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "find ~/.keras -name \"cifar-10*\" -type f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functionality to provide human-readable labels\n",
    "cifar10_label_names = ['airplane', 'automobile', \n",
    "                       'bird', 'cat', 'deer', 'dog', 'frog', 'horse', \n",
    "                       'ship', 'truck']\n",
    "\n",
    "def cifar10_index_label(idx):\n",
    "    return cifar10_label_names[idx]\n",
    "\n",
    "def cifar10_category_label(cat):\n",
    "    return cifar10_index_label(cat.argmax())\n",
    "\n",
    "def cifar10_label(v):\n",
    "    return cifar10_index_label(v) if np.isscalar(v) else cifar10_category_label(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import cifar10\n",
    "\n",
    "# The data, split between train and test sets:\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore Data\n",
    "\n",
    "Explore data types, shape, and value ranges.  Ensure they make sense, and you understand the data well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('x_train type:', type(x_train), ',', 'y_train type:', type(y_train))\n",
    "print('x_train dtype:', x_train.dtype, ',', 'y_train dtype:', y_train.dtype)\n",
    "print('x_train shape:', x_train.shape, ',', 'y_train shape:', y_train.shape)\n",
    "print('x_test shape:', x_test.shape, ',', 'y_test shape:', y_test.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('x_train (min, max, mean): (%s, %s, %s)' % (x_train.min(), x_train.max(), x_train.mean()))\n",
    "print('y_train (min, max): (%s, %s)' % (y_train.min(), y_train.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Show array of random labelled images with matplotlib (re-run cell to see new examples)\n",
    "\n",
    "fig = plt.figure(figsize=(16,8))\n",
    "\n",
    "for i in range(40):\n",
    "    plt.subplot(4, 10, i + 1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    idx = int(random.uniform(0, x_train.shape[0]))\n",
    "    plt.title(cifar10_label(y_train[idx][0]))\n",
    "    plt.imshow(x_train[idx], cmap=plt.get_cmap('gray'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "hist, bins = np.histogram(y_train, bins = range(y_train.min(), y_train.max() + 2))\n",
    "\n",
    "fig = plt.figure(figsize=(12,5))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.hist(y_train, bins = range(y_train.min(), y_train.max() + 2))\n",
    "plt.xticks(range(y_train.min(), y_train.max() + 2))\n",
    "plt.title(\"y_train histogram\")\n",
    "plt.subplot(1,2,2)\n",
    "plt.hist(x_train.flat, bins = range(x_train.min(), x_train.max() + 2))\n",
    "plt.title(\"x_train histogram\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('y_train histogram counts:', hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data looks reasonable: there are sufficient examples for each category (y_train) and a near-normal distribution of pixel values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Conversion\n",
    "\n",
    "However, the data type for the training data is `uint8`, while the input type for the network will be `float32` so the data must be converted.  Also, the data should be normalized, and the labels need to be categorical.  I.e., instead of label existing as 10 different values in a 1-D space, they need to exist as Boolean values in a 10-D space — one dimension for each category, and either a 0 or 1 value in each dimension to represent membership in that category.\n",
    "\n",
    "* https://keras.io/examples/cifar10_cnn/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = (y_train.max() - y_train.min()) + 1\n",
    "print('num_classes =', num_classes)\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "train_data = (x_train, y_train)\n",
    "test_data = (x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\n",
    "'''\n",
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "    samplewise_center=False,  # set each sample mean to 0\n",
    "    featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "    samplewise_std_normalization=False,  # divide each input by its std\n",
    "    zca_whitening=False,  # apply ZCA whitening\n",
    "    zca_epsilon=1e-06,  # epsilon for ZCA whitening\n",
    "    rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "    # randomly shift images horizontally (fraction of total width)\n",
    "    width_shift_range=0.1,\n",
    "    # randomly shift images vertically (fraction of total height)\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.,  # set range for random shear\n",
    "    zoom_range=0.,  # set range for random zoom\n",
    "    channel_shift_range=0.,  # set range for random channel shifts\n",
    "    # set mode for filling points outside the input boundaries\n",
    "    fill_mode='nearest',\n",
    "    cval=0.,  # value used for fill_mode = \"constant\"\n",
    "    horizontal_flip=True,  # randomly flip images\n",
    "    vertical_flip=False,  # randomly flip images\n",
    "    # set rescaling factor (applied before any other transformation)\n",
    "    rescale=None,\n",
    "    # set function that will be applied on each input\n",
    "    preprocessing_function=None,\n",
    "    # image data format, either \"channels_first\" or \"channels_last\"\n",
    "    data_format=None,\n",
    "    # fraction of images reserved for validation (strictly between 0 and 1)\n",
    "    validation_split=0.0)\n",
    "\n",
    "# Compute quantities required for feature-wise normalization\n",
    "# (std, mean, and principal components if ZCA whitening is applied).\n",
    "datagen.fit(x_train)\n",
    "''';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('x_train type:', type(x_train))\n",
    "print('x_train dtype:', x_train.dtype)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "print('y_train type:', type(y_train))\n",
    "print('y_train dtype:', y_train.dtype)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('y_test shape:', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acquire Pre-Trained Network\n",
    "\n",
    "Download an *ImageNet* pretrained VGG16 network[<sup>1</sup>](#fn1), sans classification layer, shaped for 32x32px colour images<sup>[*](https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5)</sup> (the smallest supported size).  This image-feature detection network is an example of a deep CNN (Convolutional Neural Network).\n",
    "\n",
    "**Note:** The network must be fixed – it was already trained on a very large dataset, so training it on our smaller dataset would result in it un-learning valuable generic features.\n",
    "\n",
    "<span id=\"fn1\"><sup>[1]</sup> Very Deep Convolutional Networks for Large-Scale Image Recognition* by Karen Simonyan and Andrew Zisserman, [arXiv (2014)](https://arxiv.org/abs/1409.1556).</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "\n",
    "conv_base = VGG16(weights='imagenet', include_top=False, input_shape=(32, 32, 3))\n",
    "conv_base.trainable = False\n",
    "conv_base.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input layer shape and data type should match with the input data:\n",
    "\n",
    "*Note:* The first dimension of the shape will differ; the input layer has `None` to indicate it accepts an a batch sized collection of arrays of the remaining shape.  The data shape has a number indicating how many samples it contains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"input layer shape:\", conv_base.layers[0].input.shape)\n",
    "print(\"input layer dtype:\", conv_base.layers[0].input.dtype)\n",
    "conv_base.layers[0].input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"input data shape:\", x_train.shape)\n",
    "print(\"input data dtype:\", x_train.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore Convolutional Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_image_plot(image_index=None):\n",
    "    if not image_index:\n",
    "        image_index = int(random.uniform(0, x_train.shape[0]))\n",
    "\n",
    "    plt.imshow(x_train[image_index], cmap='gray')\n",
    "    plt.title(\"%s (%s)\" % (cifar10_label(y_train[image_index]), image_index))\n",
    "    plt.show()\n",
    "    \n",
    "    return image_index\n",
    "\n",
    "def get_model_layer(model, layer_name):\n",
    "    if type(layer_name) == str:\n",
    "        layer = model.get_layer(layer_name)\n",
    "    else:\n",
    "        m = model\n",
    "        for ln in layer_name:\n",
    "            model = m\n",
    "            m = m.get_layer(ln)\n",
    "        layer = m\n",
    "    return (model, layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_conv_layer_weights(model, layer_name):\n",
    "    (model, layer) = get_model_layer(model, layer_name)\n",
    "    layer_weights = layer.weights[0]\n",
    "\n",
    "    max_size = layer_weights.shape[3]\n",
    "    col_size = 12\n",
    "    row_size = int(np.ceil(float(max_size) / float(col_size)))\n",
    "\n",
    "    print(\"conv layer: %s shape: %s size: (%s,%s) count: %s\" % \n",
    "          (layer_name,\n",
    "           layer_weights.shape,\n",
    "           layer_weights.shape[0], layer_weights.shape[1],\n",
    "           max_size))\n",
    "\n",
    "    fig, ax = plt.subplots(row_size,col_size,figsize=(12, 1.2 * row_size))\n",
    "    idx = 0\n",
    "\n",
    "    for row in range(0,row_size):\n",
    "        for col in range(0,col_size):\n",
    "            ax[row][col].set_xticks([])\n",
    "            ax[row][col].set_yticks([])\n",
    "            if idx < max_size:\n",
    "                ax[row][col].imshow(layer_weights[:, :, 0, idx], cmap='gray')\n",
    "            else:\n",
    "                fig.delaxes(ax[row][col])\n",
    "            idx += 1\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_conv_layer_output(model, layer_name, image_index=None):\n",
    "    (model, layer) = get_model_layer(model, layer_name)\n",
    "    layer_output = layer.output\n",
    "\n",
    "    if not image_index:\n",
    "        image_index = train_image_plot()\n",
    "        \n",
    "    intermediate_model = keras.models.Model(inputs = model.input, outputs=layer_output) \n",
    "    intermediate_prediction = intermediate_model.predict(x_train[image_index].reshape(1,32,32,3))\n",
    "  \n",
    "    max_size = layer_output.shape[3]\n",
    "    col_size = 10\n",
    "    row_size = int(np.ceil(float(max_size) / float(col_size)))\n",
    "\n",
    "    print(\"conv layer: %s shape: %s size: (%s,%s) count: %s\" % \n",
    "          (layer_name,\n",
    "           layer_output.shape,\n",
    "           layer_output.shape[1], layer_output.shape[2],\n",
    "           max_size))\n",
    "    \n",
    "    fig, ax = plt.subplots(row_size,col_size,figsize=(12, 1.2 * row_size))\n",
    "    idx = 0\n",
    "\n",
    "    for row in range(0,row_size):\n",
    "        for col in range(0,col_size):\n",
    "            ax[row][col].set_xticks([])\n",
    "            ax[row][col].set_yticks([])\n",
    "            if idx < max_size:\n",
    "                ax[row][col].imshow(intermediate_prediction[0, :, :, idx], cmap='gray')\n",
    "            else:\n",
    "                fig.delaxes(ax[row][col])\n",
    "            idx += 1\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "\n",
    "def generate_response_pattern(model, conv_layer_output, filter_index=0):\n",
    "    image_size = 32\n",
    "    #step_size = 1.0\n",
    "    epsilon = 1e-5\n",
    "\n",
    "    def process_image(x):\n",
    "        # Normalizes the tensor: centers on 0, ensures that std is 0.1 Clips to [0, 1]\n",
    "        x -= x.mean()\n",
    "        x /= (x.std() + epsilon)\n",
    "        x *= 0.1\n",
    "        x += 0.5\n",
    "        x = np.clip(x, 0, 1)\n",
    "        x *= 255\n",
    "        x = np.clip(x, 0, 255).astype('uint8')\n",
    "        return x\n",
    "\n",
    "    # TODO: is this required?\n",
    "    #with tf.device('/gpu:0'):\n",
    "    img_tensor = tf.Variable(tf.random.uniform((1, 32, 32, 3)) * 20 + 128.0, trainable=True)\n",
    "\n",
    "    response_model = keras.models.Model([model.inputs], [conv_layer_output])\n",
    "\n",
    "    for i in range(40):\n",
    "        with tf.GradientTape() as gtape:\n",
    "            layer_output = response_model(img_tensor)\n",
    "            loss = K.mean(layer_output[0, :, :, filter_index])\n",
    "            grads = gtape.gradient(loss, img_tensor)\n",
    "            grads /= (K.sqrt(K.mean(K.square(grads))) + epsilon)\n",
    "        img_tensor = tf.Variable(tf.add(img_tensor, grads))\n",
    "\n",
    "    img = np.array(img_tensor[0])\n",
    "    return process_image(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_conv_layer_response(model, layer_name):\n",
    "    (model, layer) = get_model_layer(model, layer_name)\n",
    "    layer_output = layer.output\n",
    "    \n",
    "    max_size = layer_output.shape[3]\n",
    "    col_size = 12\n",
    "    row_size = int(np.ceil(float(max_size) / float(col_size)))\n",
    "\n",
    "    print(\"conv layer: %s shape: %s size: (%s,%s) count: %s\" % \n",
    "          (layer_name,\n",
    "           layer_output.shape,\n",
    "           layer_output.shape[1], layer_output.shape[2],\n",
    "           max_size))\n",
    "    \n",
    "    fig, ax = plt.subplots(row_size,col_size,figsize=(12, 1.2 * row_size))\n",
    "    idx = 0\n",
    "\n",
    "    for row in range(0,row_size):\n",
    "        for col in range(0,col_size):\n",
    "            ax[row][col].set_xticks([])\n",
    "            ax[row][col].set_yticks([])\n",
    "            if idx < max_size:\n",
    "                img = generate_response_pattern(model, layer_output, idx)\n",
    "                ax[row][col].imshow(img, cmap='gray')\n",
    "                ax[row][col].set_title(\"%s\" % idx)\n",
    "            else:\n",
    "                fig.delaxes(ax[row][col])\n",
    "            idx += 1\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for n in [l.name for l in conv_base.layers if isinstance(l, keras.layers.Conv2D)][:4]:\n",
    "    visualize_conv_layer_weights(conv_base, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "image_index = train_image_plot()\n",
    "for n in [l.name for l in conv_base.layers if isinstance(l, keras.layers.Conv2D)][:7]:\n",
    "    visualize_conv_layer_output(conv_base, n, image_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for n in [l.name for l in conv_base.layers if isinstance(l, keras.layers.Conv2D)][:4]:\n",
    "    visualize_conv_layer_response(conv_base, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# NOTE: Visualize mid to higher level convolutional layers; \n",
    "#       lengthy operation, be prepared to wait...\n",
    "for n in [l.name for l in conv_base.layers if isinstance(l, keras.layers.Conv2D)][4:]:\n",
    "    visualize_conv_layer_response(conv_base, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Base + Classifier Model\n",
    "\n",
    "Create a simple model that has the pre-trained CNN (Convolutional Neural Network) as a base, and adds a basic classifier on top.\n",
    "\n",
    "Notice the split of total parameters (\\~15 million) between trainable (\\~0.3 million for our classifier) and non-trainable (\\~14.7 million for the pre-trained CNN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense, Activation, Dropout\n",
    "\n",
    "model = Sequential()\n",
    "model.add(conv_base)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128 #32\n",
    "epochs = 25 #100\n",
    "learning_rate = 1e-3 #1e-4\n",
    "decay = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(learning_rate=learning_rate, decay=decay),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#history = model.fit_generator(train_data,\n",
    "history = model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(x_test, y_test),\n",
    "          shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize accuracy and loss for training and validation.\n",
    "\n",
    "* https://keras.io/visualization/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def history_plot(history):\n",
    "    fig = plt.figure(figsize=(12,5))\n",
    "\n",
    "    plt.title('Model accuracy & loss')\n",
    "\n",
    "    # Plot training & validation accuracy values\n",
    "    ax1 = fig.add_subplot()\n",
    "    #ax1.set_ylim(0, 1.1 * max(history.history['loss']+history.history['val_loss']))\n",
    "    ax1.set_prop_cycle(color=['green', 'red'])\n",
    "    p1 = ax1.plot(history.history['loss'], label='Train Loss')\n",
    "    p2 = ax1.plot(history.history['val_loss'], label='Test Loss')\n",
    "\n",
    "    # Plot training & validation loss values\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.set_ylim(0, 1.1 * max(history.history['accuracy']+history.history['val_accuracy']))\n",
    "    ax2.set_prop_cycle(color=['blue', 'orange'])\n",
    "    p3 = ax2.plot(history.history['accuracy'], label='Train Acc')\n",
    "    p4 = ax2.plot(history.history['val_accuracy'], label='Test Acc')\n",
    "\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "\n",
    "    pz = p3 + p4 + p1 + p2\n",
    "    plt.legend(pz, [l.get_label() for l in pz], loc='center right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#history.history.keys()\n",
    "history_plot(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score trained model.\n",
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_plot(model, test_data):\n",
    "    (x_test, y_test) = test_data\n",
    "    fig = plt.figure(figsize=(16,8))\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for i in range(40):\n",
    "        plt.subplot(4, 10, i + 1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        idx = int(random.uniform(0, x_test.shape[0]))\n",
    "        result = model.predict_classes(x_test[idx:idx+1])[0]\n",
    "        rCorrect = True if cifar10_label(y_test[idx]) == cifar10_label(result) else False\n",
    "        rSym = '✔' if rCorrect else '✘'\n",
    "        correct += 1 if rCorrect else 0\n",
    "        total += 1\n",
    "        plt.title(\"%s %s\" % (rSym, cifar10_label(result)))\n",
    "        plt.imshow(x_test[idx], cmap=plt.get_cmap('gray'))\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"% 3.2f%% correct (%s/%s)\" % (100.0 * float(correct) / float(total), correct, total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_proba_plot(model, test_data):\n",
    "    (x_test, y_test) = test_data\n",
    "    fig = plt.figure(figsize=(15,15))\n",
    "    \n",
    "    for i in range(10):\n",
    "        plt.subplot(10, 2, (2*i) + 1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        idx = int(random.uniform(0, x_test.shape[0]))\n",
    "        result = model.predict_proba(x_test[idx:idx+1])[0] * 100 # prob -> percent\n",
    "        plt.title(\"%s (%s)\" % (cifar10_label(y_test[idx]), idx))\n",
    "        plt.imshow(x_test[idx], cmap=plt.get_cmap('gray'))\n",
    "        \n",
    "        ax = plt.subplot(10, 2, (2*i) + 2)\n",
    "        plt.bar(np.arange(len(result)), result, label='%')\n",
    "        plt.xticks(range(0, len(result) + 1))\n",
    "        ax.set_xticklabels(cifar10_label_names)\n",
    "        plt.title(\"classifier probabilities\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prediction_plot(model, (x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prediction_proba_plot(model, (x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Classifier Model\n",
    "\n",
    "Create a basic CNN (Convolutional Neural Network) based classifier from scratch.\n",
    "\n",
    "Notice the split of total parameters (\\~15 million) between trainable (\\~0.3 million for our classifier) and non-trainable (\\~14.7 million for the pre-trained CNN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense, Activation, Dropout, Conv2D, MaxPooling2D\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), padding='same',\n",
    "                 input_shape=x_train.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128 #32\n",
    "epochs = 25 #100\n",
    "learning_rate = 1e-3 #1e-4\n",
    "decay = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(learning_rate=learning_rate, decay=decay),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#history = model.fit_generator(train_data,\n",
    "history = model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(x_test, y_test),\n",
    "          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "history_plot(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score trained model.\n",
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prediction_plot(model, (x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prediction_proba_plot(model, (x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for n in [l.name for l in model.layers if isinstance(l, keras.layers.Conv2D)][:4]:\n",
    "    visualize_conv_layer_weights(model, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "image_index = train_image_plot()\n",
    "for n in [l.name for l in model.layers if isinstance(l, keras.layers.Conv2D)]:\n",
    "    visualize_conv_layer_output(model, n, image_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in [l.name for l in model.layers if isinstance(l, keras.layers.Conv2D)][:4]:\n",
    "    visualize_conv_layer_response(model, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: In progress..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "print(history.history.keys())\n",
    "max(history.history['val_loss']+history.history['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Data Agumentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Resize Data to (150, 150, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Create joined model (combine two models above)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Mixed Precision\n",
    "* https://www.tensorflow.org/guide/keras/mixed_precision\n",
    "* https://developer.nvidia.com/automatic-mixed-precision\n",
    "* https://docs.nvidia.com/deeplearning/sdk/mixed-precision-training/index.html\n",
    "\n",
    "```python\n",
    "opt = tf.train.experimental.enable_mixed_precision_graph_rewrite(opt) \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Multi-GPU Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Multi-GPU Example\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense, Activation, Dropout\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "batch_size = 2 * 32\n",
    "epochs = 100\n",
    "learning_rate = 2 * 2e-5\n",
    "\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "with strategy.scope():\n",
    "    conv_base = VGG16(weights='imagenet', include_top=False, input_shape=(32, 32, 3))\n",
    "    conv_base.trainable = False\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(conv_base)\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(learning_rate=learning_rate),\n",
    "              metrics=['acc'])\n",
    "\n",
    "    history = model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(x_test, y_test),\n",
    "          shuffle=True)\n",
    "\n",
    "#conv_base.summary()\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speaker Bios\n",
    "\n",
    "Glendon Holst is a Staff Scientist in the Visualization Core Lab at KAUST (King Abdullah University of Science and Technology) specializing in HPC workflow solutions for deep learning, image processing, and scientific visualization.\n",
    "\n",
    "Mohsin Ahmed Shaikh is a Computational Scientist in the Supercomputing Core Lab at KAUST (King Abdullah University of Science and Technology) specializing in large scale HPC applications and GPGPU support for users on Ibex (cluster) and Shaheen (supercomputer).  Mohsin holds a PhD in Computational Bioengineering, and a Post Doc, from University of Canterbury, New Zealand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "* https://qz.com/1034972/the-data-that-changed-the-direction-of-ai-research-and-possibly-the-world/\n",
    "* https://www.cs.toronto.edu/~kriz/cifar.html\n",
    "* http://yann.lecun.com/exdb/mnist/index.html\n",
    "* https://towardsdatascience.com/transfer-learning-from-pre-trained-models-f2393f124751 <br/>\n",
    "  https://towardsdatascience.com/keras-transfer-learning-for-beginners-6c9b8b7143e\n",
    "  https://machinelearningmastery.com/how-to-improve-performance-with-transfer-learning-for-deep-learning-neural-networks/\n",
    "* https://arxiv.org/abs/1409.1556\n",
    "* https://www.kaggle.com/c/digit-recognizer\n",
    "* https://jupyter-notebook.readthedocs.io/en/stable/\n",
    "* https://github.com/kaust-vislab/handson-ml2\n",
    "* https://keras.io/examples/cifar10_cnn/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTING / In-Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x0 = tf.get_variable('x0', shape=(), dtype=tf.float32)\n",
    "#x1 = tf.constant(3.)\n",
    "#x = x0 + x1\n",
    "x = tf.constant(3.0)\n",
    "y = tf.constant(4.0)\n",
    "\n",
    "img0 = tf.Variable(tf.zeros((1, 32, 32, 3)), trainable=True)\n",
    "img = tf.Variable(tf.random.uniform((1, 32, 32, 3)) * 20 + 128.0, trainable=True)\n",
    "\n",
    "gtape = tf.GradientTape(persistent=True)\n",
    "with gtape as tape:\n",
    "    tape.watch([img0, img, m.input, l_out, m.output, model_bp.input, model_bp.output])\n",
    "    r0 = m.call(img0)\n",
    "    print('img0', K.mean(img0), type(img0), img0.shape, img0.dtype)\n",
    "    print('r0', K.mean(r0), type(r0), r0.shape, r0.dtype)\n",
    "    print('l_out', K.mean(l_out), type(l_out), l_out.shape, l_out.dtype)\n",
    "    r1 = m.call(img)\n",
    "    print('img', K.mean(img), type(img), img.shape, img.dtype)\n",
    "    print('r1', K.mean(r1), type(r1), r1.shape, r1.dtype)\n",
    "    print('l_out', K.mean(l_out), type(l_out), l_out.shape, l_out.dtype)\n",
    "    r2 = model_bp(img)\n",
    "    print('r2', K.mean(r2), type(r2), r2.shape, r2.dtype)\n",
    "    print('l_out', K.mean(l_out), type(l_out), l_out.shape, l_out.dtype)\n",
    "    #m.input(img)\n",
    "    #m.layers[0](img)\n",
    "    loss = K.mean(l_out[0, :, :, 0])\n",
    "    #r = m.predict(img)\n",
    "    z = img + img\n",
    "\n",
    "\n",
    "gradzxy = tape.gradient(l_out, img)\n",
    "print(\"gradzxy\", gradzxy)\n",
    "\n",
    "loss = K.mean(l_out[0, :, :, 0])\n",
    "#grads = tape.gradient(loss, m.input)\n",
    "#grads = tape.gradient(l_out, m.input)\n",
    "print(\"m.input:\", m.input[0,:,:,:])\n",
    "print(\"loss:\", loss, loss.shape, loss.dtype)\n",
    "#print(\"grads\", grads)\n",
    "#gtape.gradient?\n",
    "#conv_base_bp.trainable_variables\n",
    "#conv_base.variables\n",
    "print(m.layers[0])\n",
    "print(m.input)\n",
    "m.layers[0](img)\n",
    "l_out[0, :, :, 0]\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* https://stackoverflow.com/questions/58322147/how-to-generate-cnn-heatmaps-using-built-in-keras-in-tf2-0-tf-keras\n",
    "* https://gist.github.com/haimat/10a53ad9675f8f5ac1290f06c3e4f973"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "from tensorflow.keras import preprocessing\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import models\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "image_size = 32\n",
    "\n",
    "# Load pre-trained Keras model and the image to classify\n",
    "#model = tf.keras.applications.vgg16.VGG16()\n",
    "model = VGG16(weights='imagenet', include_top=False, input_shape=(32, 32, 3))\n",
    "\n",
    "img_tensor = tf.Variable(tf.random.uniform((1, 32, 32, 3)) * 20 + 128.0, trainable=True)\n",
    "print(type(img_tensor), img_tensor.shape, img_tensor.dtype)\n",
    "\n",
    "conv_layer = model.get_layer(\"block5_conv3\")\n",
    "heatmap_model = models.Model([model.inputs], [conv_layer.output, model.output])\n",
    "\n",
    "# Get gradient of the winner class w.r.t. the output of the (last) conv. layer\n",
    "with tf.GradientTape() as gtape:\n",
    "    conv_output, predictions = heatmap_model(img_tensor)\n",
    "    print(type(conv_output), type(predictions))\n",
    "    print(conv_output.shape, predictions.shape, np.argmax(predictions[0]))\n",
    "    loss = predictions[:, :, :, np.argmax(predictions[0])]\n",
    "    grads = gtape.gradient(loss, conv_output)\n",
    "    #grads = gtape.gradient(loss, img_tensor)\n",
    "    #grads = gtape.gradient(conv_output, img_tensor)\n",
    "    pooled_grads = K.mean(grads, axis=(0, 1, 2))\n",
    "\n",
    "print(\"grads\", grads)\n",
    "\n",
    "heatmap = tf.reduce_mean(tf.multiply(pooled_grads, conv_output), axis=-1)\n",
    "heatmap = np.maximum(heatmap, 0)\n",
    "max_heat = np.max(heatmap)\n",
    "if max_heat == 0:\n",
    "    max_heat = 1e-10\n",
    "heatmap /= max_heat\n",
    "\n",
    "print(heatmap.shape)\n",
    "print(heatmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(x):\n",
    "    # Normalizes the tensor: centers on 0, ensures that std is 0.1 Clips to [0, 1]\n",
    "    x -= x.mean()\n",
    "    x /= (x.std() + epsilon)\n",
    "    x *= 0.1\n",
    "    x += 0.5\n",
    "    x = np.clip(x, 0, 1)\n",
    "    x *= 255\n",
    "    x = np.clip(x, 0, 255).astype('uint8')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.applications import vgg16\n",
    "from tensorflow.keras import preprocessing\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "def _watch_layer(layer, tape):\n",
    "    def decorator(func):\n",
    "        def wrapper(*args, **kwargs):\n",
    "            # Store the result of `layer.call` internally.\n",
    "            layer.result = func(*args, **kwargs)\n",
    "            # From this point onwards, watch this tensor.\n",
    "            tape.watch(layer.result)\n",
    "            # Return the result to continue with the forward pass.\n",
    "            return layer.result\n",
    "\n",
    "        return wrapper\n",
    "\n",
    "    layer.call = decorator(layer.call)\n",
    "    return layer\n",
    "\n",
    "\n",
    "def _deprocess_tensor_to_image(tensor):\n",
    "    # Normalize the tensor: centers on 0, ensures that std is 0.1\n",
    "    tensor -= tensor.mean()\n",
    "    tensor /= (tensor.std()) + 1e-5\n",
    "    tensor *= 0.1\n",
    "\n",
    "    # Clip to [0,1]\n",
    "    tensor += 0.5\n",
    "    tensor = np.clip(tensor, 0, 1)\n",
    "\n",
    "    # Converts to an RGB array\n",
    "    tensor *= 255\n",
    "    tensor = np.clip(tensor, 0, 255).astype(\"uint8\")\n",
    "\n",
    "    return tensor\n",
    "\n",
    "\n",
    "image_size = 150\n",
    "image_path = \"/tmp/images/test-image.jpg\"\n",
    "\n",
    "# Create Keras model from pre-trained VGG16 and custom classifier\n",
    "input_layer = layers.Input(shape=(image_size, image_size, 3), name=\"model_input\")\n",
    "vgg16_model = VGG16(weights=\"imagenet\", include_top=False, input_tensor=input_layer)\n",
    "model_head = vgg16_model.output\n",
    "model_head = layers.Flatten(name=\"model_head_flatten\")(model_head)\n",
    "model_head = layers.Dense(256, activation=\"relu\")(model_head)\n",
    "model_head = layers.Dense(3, activation=\"softmax\")(model_head)\n",
    "model = models.Model(inputs=input_layer, outputs=model_head)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=optimizers.Adam(), metrics=[\"accuracy\"])\n",
    "\n",
    "# Load image to classify\n",
    "image = preprocessing.image.load_img(image_path, target_size=(image_size, image_size))\n",
    "img_tensor = preprocessing.image.img_to_array(image)\n",
    "img_tensor = np.expand_dims(img_tensor, axis=0)\n",
    "img_tensor = vgg16.preprocess_input(img_tensor)\n",
    "\n",
    "# Get the gradient of the winner class with regard to the output of the (last) conv. layer\n",
    "conv_layer = model.get_layer(\"block5_conv3\")\n",
    "with tf.GradientTape() as gtape:\n",
    "    _watch_layer(conv_layer, gtape)\n",
    "    preds = model.predict(img_tensor)\n",
    "    model_prediction = model.output[:, np.argmax(preds[0])]\n",
    "    grads = gtape.gradient(model_prediction, conv_layer.output)\n",
    "\n",
    "pooled_grads = K.mean(grads, axis=(0, 1, 2))\n",
    "\n",
    "# Get values of pooled grads and model conv. layer output as Numpy arrays\n",
    "iterate = K.function([input_layer], [pooled_grads, conv_layer.output[0]])\n",
    "pooled_grads_value, conv_layer_output_value = iterate([img_tensor])\n",
    "\n",
    "# Multiply each channel in the feature-map array by \"how important this channel is\"\n",
    "for i in range(pooled_grads_value.shape[0]):\n",
    "    conv_layer_output_value[:, :, i] *= pooled_grads_value[i]\n",
    "\n",
    "# The channel-wise mean of the resulting feature map is the heatmap of the class activation\n",
    "heatmap = np.mean(conv_layer_output_value, axis=-1)\n",
    "heatmap = np.maximum(heatmap, 0)\n",
    "max_heat = np.max(heatmap)\n",
    "if max_heat == 0:\n",
    "    max_heat = 1e-10\n",
    "heatmap /= max_heat\n",
    "\n",
    "'''\n",
    "# Load image via CV2\n",
    "image = cv2.imread(image_path)\n",
    "\n",
    "# Resize heatmap to original image size, normalize it, convert to RGB, apply color map\n",
    "heatmap = cv2.resize(heatmap, (image.shape[1], image.shape[0]))\n",
    "heatmap = cv2.normalize(heatmap, heatmap, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
    "heatmap = cv2.applyColorMap(np.uint8(255 * (255 - heatmap)), cv2.COLORMAP_JET)\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
