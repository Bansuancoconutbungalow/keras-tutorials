{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning from Pre-Trained Models with Keras\n",
    "\n",
    "## Introduction\n",
    "\n",
    "ImageNet, an image recognition benchmark dataset*, helped trigger the modern AI explosion.  In 2012, the AlexNet architecture (a deep convolutional-neural-network) rocked the ImageNet benchmark competition, handily beating the next best entrant.  By 2014, all the leading competitors were deep learning based.  Since then, accuracy scores continued to improve, eventually surpassing human performance.\n",
    "\n",
    "In this hands-on tutorial we will build on this pioneering work to create our own neural-network architecture for image recognition.  Participants will use the elegant Keras deep learning programming interface to build and train TensorFlow models for image classification tasks on the CIFAR-10 / MNIST datasets*.  We will demonstrate the use of transfer learning* (to give our networks a head-start by building on top of existing, ImageNet pre-trained, network layers*), and explore how to improve model performance for standard deep learning pipelines.  We will use cloud-based interactive Jupyter notebooks to work through our explorations step-by-step.  Once participants have successfully trained their custom model we will show them how to submit their model's predictions to Kaggle for scoring*.\n",
    "\n",
    "This tutorial aims to prepare participants for the HPC Saudi 2020 Student AI Competition.\n",
    "\n",
    "Participants are expected to bring their own laptops and sign-up for free online cloud services (e.g., Google Colab, Kaggle).  They may also need to download free, open-source software prior to arriving for the workshop.\n",
    "\n",
    "This tutorial assumes some basic knowledge of neural networks. If you’re not already familiar with neural networks, then you can learn the basics concepts behind neural networks at [course.fast.ai](https://course.fast.ai/).\n",
    "\n",
    "* Tutorial materials are derived from:\n",
    "  * [PyTorch Tutorials](https://github.com/kaust-vislab/pytorch-tutorials) by David Pugh.\n",
    "  * [What is torch.nn really?](https://pytorch.org/tutorials/beginner/nn_tutorial.html) by Jeremy Howard, Rachel Thomas, Francisco Ingham.\n",
    "  * [Machine Learning Notebooks](https://github.com/ageron/handson-ml2) (2nd Ed.) by Aurélien Géron.\n",
    "  * *Deep Learning with Python* by François Chollet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jupyter Notebooks\n",
    "\n",
    "This is a Jupyter Notebook.  It provides a simple, cell-based, IDE for developing and exploring complex ideas via code, visualizations, and documentation.\n",
    "\n",
    "A notebook has two primary types of cells: i) `markdown` cells for textual notes and documentation, such as the one you are reading now, and ii) `code` cells, which contain snippets of code (typically *Python*, but also *bash* scripts) that can be executed.  \n",
    "\n",
    "The currently selected cell appears within a box. A green box indicates that the cell is editable.  Clicking inside a *code* cell makes it selected and editable.  Double-click inside *markdown* cells to edit.\n",
    "\n",
    "Use `Tab` for context-sensitive code-completion assistance when editing Python code in *code* cells.  For example, use code assistance after a `.` seperator to find available object members.  For help documentation, create a new *code* cell, and use commands like `dir(`*module*`)`, `help(`*topic*`)`, `?`*name*, or `??`*function* for user provided *module*, *topic*, variable *name*, or *function* name.  The magic `?` and `??` commands show documentation / source code in a separate pane.\n",
    "\n",
    "Clicking on `[Run]` or pressing `Ctrl-Enter` will execute the contents of a cell.  A *markdown* cell converts to its display version, and a *code* cell runs the code inside.  To the left of a *code* cell is a small text bracket `In [ ]:`.  If the bracket contains an asterix, e.g., `In [*]:`, that cell is currently executing.  Only one cell executes at a time (if multiple cells are *Run*, they are queued up to execute in the order they were run).  When a *code* cell finishes executing, the bracket shows an execution count in the bracket – each *code* cell execution increments the counter and provides a way to determine the order in which codes were executed – e.g., `In [7]` for the seventh cell to complete.  \n",
    "\n",
    "The output produced by a *code* cell appears at the bottom of that cell after it executes.  The output generated by a code cell includes anything printed to the output during execution (e.g., print statements, or thrown errors) and the final value generated by the cell (i.e., not the intermediate values).  The final value is 'pretty printed' by Jupyter.\n",
    "\n",
    "Typically, notebooks are written to be executed in order, from top to bottom.  Behind the scenes, however, each Notebook has a single Python state (the `kernel`), and each *code* cell that executes, modifies that state.  It is possible to modify and re-run earlier cells; however, care must be taken to also re-run any other cells that depend upon the modified one.  List the Python state global variables with the magic command `%wgets`.  The *kernel* can be restarted to a known state, and cell output cleared, if the Python state becomes too confusing to fix manually (choose `Restart & Clear Output` from the Jupyter `Kernel` menu) – this requires running each *code* cell again.\n",
    "\n",
    "Complete user documentation is available at [jupyter-notebook.readthedocs.io](https://jupyter-notebook.readthedocs.io/en/stable/notebook.html#notebook-user-interface). <br/>\n",
    "Many helpful tips and techniques from [28 Jupyter Notebook Tips, Tricks, and Shortcuts](https://www.dataquest.io/blog/jupyter-notebook-tips-tricks-shortcuts/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Kaggle Account\n",
    "\n",
    "#### 1. Register for an account\n",
    "\n",
    "In order to download Kaggle competition data you will first need to create a [Kaggle](https://www.kaggle.com/) account.\n",
    "\n",
    "#### 2. Create an API key\n",
    "\n",
    "Once you have registered for a Kaggle account you will need to create [API credentials](https://github.com/Kaggle/kaggle-api#api-credentials) in order to be able to use the `kaggle` CLI to download data.\n",
    "\n",
    "* Go to the `Account` tab of your user profile, \n",
    "* and click `Create New API Token` from the API section.  \n",
    "\n",
    "This generates a `kaggle.json` file (with 'username' and 'key' values) to download.\n",
    "\n",
    "\n",
    "### Setup Colab\n",
    "\n",
    "In order to run this notebook in [Google Colab](https://colab.research.google.com) you will need a [Google Account](https://accounts.google.com/).  Sign-in to your Google account, if necessary, and then start the notebook.\n",
    "\n",
    "Change Google Colab runtime to use GPU:\n",
    "\n",
    "* Click `Runtime` -> `Change runtime type` menu item\n",
    "* Specify `Runtime type` as `Python 3`\n",
    "* Specify `Hardware accelerator` as `GPU`\n",
    "* Click **[Save]** button\n",
    "\n",
    "The session indicator (toolbar / status ribbon under menu) should briefly appear as `Connecting...`.  When the session restarts, continue with the next cell (specifying TensorFlow version v2.x):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  # %tensorflow_version only exists in Colab.\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "  pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import tensorflow.keras.utils as Kutils\n",
    "\n",
    "def cache_mnist_data():\n",
    "    for n in [\"mnist.npz\", \"kaggle/train.csv\", \"kaggle/test.csv\"]:\n",
    "        path = pathlib.Path(\"../datasets/mnist/%s\" % n).absolute()\n",
    "        DATA_URL = \"file:///\" + str(path)\n",
    "        data_file_path = Kutils.get_file(n.replace('/','-mnist-'), DATA_URL)\n",
    "        print(\"cached file: %s\" % n)\n",
    "\n",
    "def cache_cifar10_data():\n",
    "    for n in [\"cifar-10.npz\", \"cifar-10-batches-py.tar.gz\"]:\n",
    "        path = pathlib.Path(\"../datasets/cifar10/%s\" % n).absolute()\n",
    "        DATA_URL = \"file:///\" + str(path)\n",
    "        if path.is_file(): \n",
    "            data_file_path = Kutils.get_file(n, DATA_URL)\n",
    "            print(\"cached file: %s\" % n)\n",
    "        else:\n",
    "            print(\"FAILED: First fetch file: %s\" % n)\n",
    "\n",
    "def cache_models():\n",
    "    for n in [\"vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\"]:\n",
    "        path = pathlib.Path(\"../models/%s\" % n).absolute()\n",
    "        DATA_URL = \"file:///\" + str(path)\n",
    "        if path.is_file(): \n",
    "            data_file_path = Kutils.get_file(n, DATA_URL, cache_subdir='models')\n",
    "            print(\"cached file: %s\" % n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download MNIST Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are using Binder to run this notebook, then the data is already downloaded and available.  Skip to the next step.\n",
    "\n",
    "If you are using Google Colab to run this notebook, then you will need to download the data before proceeding.\n",
    "\n",
    "##### Download MNIST from Kaggle\n",
    "\n",
    "**Note:** Before attempting to download the competition data you will need to login to your [Kaggle](https://www.kaggle.com) account and accept the rules for this competition.\n",
    "\n",
    "Set your Kaggle username and API key (from the `kaggle.json` file) into the cell below, and execute the code to download the Kaggle [Digit Recognizer: Learn computer vision with the famous MNIST data](https://www.kaggle.com/c/digit-recognizer) competition data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# NOTE: Replace YOUR_USERNAME and YOUR_API_KEY with actual credentials \n",
    "export KAGGLE_USERNAME=\"YOUR_USERNAME\"\n",
    "export KAGGLE_KEY=\"YOUR_API_KEY\"\n",
    "kaggle competitions download -c digit-recognizer -p ../datasets/mnist/kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "unzip ../datasets/mnist/kaggle/digit-recognizer.zip -d ../datasets/mnist/kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (Alternative) Download MNIST from GitHub\n",
    "\n",
    "If you are running this notebook using Google Colab, but did *not* create a Kaggle account and API key, then  dowload the data from our GitHub repository by running the code in the following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import requests\n",
    "\n",
    "def fetch_mnist_data():\n",
    "    RAW_URL = \"https://github.com/holstgr-kaust/keras-tutorials/raw/master/datasets/mnist\"\n",
    "    DEST_DIR = pathlib.Path('../datasets/mnist')\n",
    "\n",
    "    DEST_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    for n in [\"mnist.npz\", \"kaggle/train.csv\", \"kaggle/test.csv\", \"kaggle/sample_submission.csv\"]:\n",
    "        path = DEST_DIR / n\n",
    "        if not path.is_file():  # Don't download if file exists\n",
    "            with path.open(mode = 'wb') as f:\n",
    "                response = requests.get(RAW_URL + \"/\" + n)\n",
    "                f.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch_mnist_data()\n",
    "cache_mnist_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (Alternative) Download MNIST with Keras\n",
    "\n",
    "If you are running this notebook using Google Colab, but did *not* create a Kaggle account and API key, then dowload the data using the Keras load_data() API by running the code in the following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "cache_mnist_data()\n",
    "mnist.load_data();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download CIFAR10 Data\n",
    "\n",
    "If you are using Binder to run this notebook, then the data is already downloaded and available.  Skip to the next step.\n",
    "\n",
    "If you are using Google Colab to run this notebook, then you will need to download the data before proceeding.\n",
    "\n",
    "##### Download CIFAR10 from Kaggle\n",
    "\n",
    "**Note:** Before attempting to download the competition data you will need to login to your [Kaggle](https://www.kaggle.com) account.\n",
    "\n",
    "Set your Kaggle username and API key (from the `kaggle.json` file) into the cell below, and execute the code to download the Kaggle [Digit Recognizer: Learn computer vision with the famous MNIST data](https://www.kaggle.com/c/digit-recognizer) competition data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# NOTE: Replace YOUR_USERNAME and YOUR_API_KEY with actual credentials \n",
    "export KAGGLE_USERNAME=\"YOUR_USERNAME\"\n",
    "export KAGGLE_KEY=\"YOUR_API_KEY\"\n",
    "kaggle datasets download guesejustin/cifar10-keras-files-cifar10load-data -p ../datasets/cifar10/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "unzip ../datasets/cifar10/cifar10-keras-files-cifar10load-data.zip -d ../datasets/cifar10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (Alternative) Download CIFAR10 from GitHub\n",
    "\n",
    "If you are running this notebook using Google Colab, but did *not* create a Kaggle account and API key, then  dowload the data from our GitHub repository by running the code in the following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import requests\n",
    "\n",
    "def fetch_cifar10_data():\n",
    "    RAW_URL = \"https://github.com/holstgr-kaust/keras-tutorials/raw/master/datasets/cifar10\"\n",
    "    DEST_DIR = pathlib.Path('../datasets/cifar10')\n",
    "\n",
    "    DEST_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    for n in [\"cifar-10.npz\", \"cifar-10-batches-py.tar.gz\"]:\n",
    "        path = DEST_DIR / n\n",
    "        if not path.is_file():  # Don't download if file exists\n",
    "            with path.open(mode = 'wb') as f:\n",
    "                response = requests.get(RAW_URL + \"/\" + n)\n",
    "                f.write(response.content)\n",
    "            print(\"downloaded file: %s\" % n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch_cifar10_data()\n",
    "cache_cifar10_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "DEST_DIR='../datasets/cifar10'\n",
    "tar xvpf \"${DEST_DIR}/cifar-10-batches-py.tar.gz\" --directory=\"${DEST_DIR}\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (Alternative) Download CIFAR10 with Keras\n",
    "\n",
    "If you are running this notebook using Google Colab, but did *not* create a Kaggle account and API key, then dowload the data using the Keras load_data() API by running the code in the following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import cifar10\n",
    "cache_cifar10_data()\n",
    "cifar10.load_data();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial\n",
    "\n",
    "### Setup\n",
    "\n",
    "Initialize the Python environment by importing and verifying the modules we will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pathlib\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`%matplotlib inline` is a magic command that makes *matplotlib* charts and plots appear was outputs in the notebook.\n",
    "\n",
    "`%matplotlib notebook` enables semi-interactive plots that can be enlarged, zoomed, and cropped while the plot is active.  One issue with this option is that new plots appear in the active plot widget, not in the cell where the data was produced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify runtime environment\n",
    "\n",
    "try:\n",
    "    # %tensorflow_version only exists in Colab.\n",
    "    %tensorflow_version 2.x\n",
    "    IS_COLAB = True\n",
    "except Exception:\n",
    "    IS_COLAB = False\n",
    "print(\"is_colab:\", IS_COLAB)\n",
    "\n",
    "assert tf.__version__ >= \"2.0\", \"TensorFlow version >= 2.0 required.\"\n",
    "print(\"tensorflow_version:\", tf.__version__)\n",
    "\n",
    "assert sys.version_info >= (3, 5), \"Python >= 3.5 required.\"\n",
    "print(\"python_version:\", \"%s.%s.%s-%s\" % (sys.version_info.major, \n",
    "                                          sys.version_info.minor,\n",
    "                                          sys.version_info.micro,\n",
    "                                          sys.version_info.releaselevel\n",
    "                                         ))\n",
    "\n",
    "print(\"executing_eagerly:\", tf.executing_eagerly())\n",
    "\n",
    "__physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "if len(__physical_devices) == 0:\n",
    "    print(\"No GPUs available. Expect training to be very slow.\")\n",
    "    if IS_COLAB:\n",
    "        print(\"Go to `Runtime` > `Change runtime` and select a GPU hardware accelerator.\"\n",
    "              \"Then `Save` to restart session.\")\n",
    "else:\n",
    "    print(\"is_built_with_cuda:\", tf.test.is_built_with_cuda())\n",
    "    print(\"is_gpu_available:\", tf.test.is_gpu_available(), [d.name for d in __physical_devices])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIFAR10 - Dataset Processing\n",
    "\n",
    "The previously acquired CIFAR10 dataset is the essential input needed to train an image classification model. Before using the dataset, there are several preprocessing steps required to load the data, and create the correctly sized training, validation, and testing arrays used as input to the network.\n",
    "\n",
    "The following data preparation steps are needed before they can become inputs to the network:\n",
    "\n",
    "* Cache the downloaded dataset (to use Keras `load_data()` functionality).\n",
    "* Load the dataset (CIFAR10 is small, and fits into a `numpy` array).\n",
    "* Verify the shape and type of the data, and understand it...\n",
    "* Convert label indices into categorical vectors.\n",
    "* Convert image data from integer to float values, and normalize.\n",
    "  * Verify converted input data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cache Data\n",
    "\n",
    "Make downloaded data available to Keras.  Provide dataset utility functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cache CIFAR10 Datasets\n",
    "cache_cifar10_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "find ~/.keras -name \"cifar-10*\" -type f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functionality to provide human-readable labels\n",
    "cifar10_label_names = ['airplane', 'automobile', \n",
    "                       'bird', 'cat', 'deer', 'dog', 'frog', 'horse', \n",
    "                       'ship', 'truck']\n",
    "\n",
    "def cifar10_index_label(idx):\n",
    "    return cifar10_label_names[int(idx)]\n",
    "\n",
    "def cifar10_category_label(cat):\n",
    "    return cifar10_index_label(cat.argmax())\n",
    "\n",
    "def cifar10_label(v):\n",
    "    return cifar10_index_label(v) if np.isscalar(v) or np.size(v) == 1 else cifar10_category_label(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import cifar10\n",
    "\n",
    "# The data, split between train and test sets:\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Backup plan: Only run the following cell if the data didn't load via `cifar10.load_data` above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try secondary data source if the first didn't work\n",
    "try:\n",
    "    print(\"data loaded.\" if type((x_train, y_train, x_test, y_test)) else \"load failed...\")\n",
    "except NameError:\n",
    "    with np.load('../datasets/cifar10/cifar-10.npz') as data:\n",
    "        x_train = data['x_train']\n",
    "        y_train = data['y_train']\n",
    "        x_test = data['x_test']\n",
    "        y_test = data['y_test']\n",
    "    print(\"alternate data load.\" if type((x_train, y_train, x_test, y_test)) else \"failed...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore Data\n",
    "\n",
    "Explore data types, shape, and value ranges.  Ensure they make sense, and you understand the data well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('x_train type:', type(x_train), ',', 'y_train type:', type(y_train))\n",
    "print('x_train dtype:', x_train.dtype, ',', 'y_train dtype:', y_train.dtype)\n",
    "print('x_train shape:', x_train.shape, ',', 'y_train shape:', y_train.shape)\n",
    "print('x_test shape:', x_test.shape, ',', 'y_test shape:', y_test.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('x_train (min, max, mean): (%s, %s, %s)' % (x_train.min(), x_train.max(), x_train.mean()))\n",
    "print('y_train (min, max): (%s, %s)' % (y_train.min(), y_train.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def cifar10_imageset_plot(img_data=None):\n",
    "    (x_imgs, y_imgs) = img_data if img_data else (x_train, y_train)\n",
    "    fig = plt.figure(figsize=(16,8))\n",
    "\n",
    "    for i in range(40):\n",
    "        plt.subplot(4, 10, i + 1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        idx = int(random.uniform(0, x_imgs.shape[0]))\n",
    "        plt.title(cifar10_label(y_imgs[idx]))\n",
    "        plt.imshow(x_imgs[idx], cmap=plt.get_cmap('gray'))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show array of random labelled images with matplotlib (re-run cell to see new examples)\n",
    "cifar10_imageset_plot((x_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def histogram_plot(img_data=None):\n",
    "    (x_data, y_data) = img_data if img_data else (x_train, y_train)\n",
    "    \n",
    "    hist, bins = np.histogram(y_data, bins = range(int(y_data.min()), int(y_data.max() + 2)))\n",
    "\n",
    "    fig = plt.figure(figsize=(12,5))\n",
    "\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.hist(y_data, bins = range(int(y_data.min()), int(y_data.max() + 2)))\n",
    "    plt.xticks(range(int(y_data.min()), int(y_data.max() + 2)))\n",
    "    plt.title(\"y histogram\")\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.hist(x_data.flat, bins = range(int(x_data.min()), int(x_data.max() + 2)))\n",
    "    plt.title(\"x histogram\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print('y histogram counts:', hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram_plot((x_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram_plot((x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data looks reasonable: there are sufficient examples for each category (y_train) and a near-normal distribution of pixel values that appears similar in both the train and test datasets.\n",
    "\n",
    "##### Visualizing training samples using PCA\n",
    "\n",
    "[Principal Components Analysis (PCA)](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) can be used as a visualization tool to see if there are any obvious patterns in the training samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import sklearn.decomposition\n",
    "\n",
    "_prng = np.random.RandomState(42)\n",
    "\n",
    "pca = sklearn.decomposition.PCA(n_components=3, random_state=_prng)\n",
    "\n",
    "x_train_flat = x_train.reshape(*x_train.shape[:1], -1)\n",
    "y_train_flat = y_train.reshape(y_train.shape[0])\n",
    "print(\"x_train:\", x_train.shape, \"y_train\", y_train.shape)\n",
    "print(\"x_train_flat:\", x_train_flat.shape, \"y_train_flat\", y_train_flat.shape)\n",
    "pca_train_features = pca.fit_transform(x_train_flat, y_train_flat)\n",
    "print(\"pca_train_features:\", pca_train_features.shape)\n",
    "\n",
    "# Sample 10% of the PCA results\n",
    "_idxs = _prng.randint(y_train_flat.shape[0], size=y_train_flat.shape[0] // 10)\n",
    "pca_features = pca_train_features[_idxs]\n",
    "pca_category = y_train_flat[_idxs]\n",
    "print(\"pca_features:\", pca_features.shape, \n",
    "      \"pca_category\", pca_category.shape, \n",
    "      \"min,max category:\", pca_category.min(), pca_category.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def category_scatter_plot(features, category):\n",
    "    num_category = 1 + category.max() - category.min()\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(12, 10))\n",
    "    cm = plt.cm.get_cmap('tab10', num_category)\n",
    "    sc = ax.scatter(features[:,0], features[:,1], c=category, alpha=0.4, cmap=cm)\n",
    "    ax.set_xlabel(\"Component 1\")\n",
    "    ax.set_ylabel(\"Component 2\")\n",
    "    ax.set_title(\"CIFAR10 - PCA\")\n",
    "    plt.colorbar(sc)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "def category_scatter3d_plot(features, category):\n",
    "    num_category = 1 + category.max() - category.min()\n",
    "    mean_feat = np.mean(features, axis=0)\n",
    "    std_feat = np.std(features, axis=0)\n",
    "    min_range = mean_feat - std_feat\n",
    "    max_range = mean_feat + std_feat\n",
    "    \n",
    "    fig = plt.figure(figsize=(12, 10))\n",
    "    cm = plt.cm.get_cmap('tab10', num_category)\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    sc = ax.scatter(features[:,0], features[:,1], features[:,2],\n",
    "                    c=category, alpha=0.85, cmap=cm)\n",
    "    ax.set_xlabel(\"Component 1\")\n",
    "    ax.set_ylabel(\"Component 2\")\n",
    "    ax.set_zlabel(\"Component 3\")\n",
    "    ax.set_title(\"CIFAR10 - PCA\")\n",
    "    ax.set_xlim(2.0 * min_range[0], 2.0 * max_range[0])\n",
    "    ax.set_ylim(2.0 * min_range[1], 2.0 * max_range[1])\n",
    "    ax.set_zlim(2.0 * min_range[2], 2.0 * max_range[2])\n",
    "    plt.colorbar(sc)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "category_scatter_plot(pca_features, pca_category)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** 3D PCA plot works best with `%matplotlib notebook` to enable interactive rotation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_scatter3d_plot(pca_features, pca_category)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data in its original image space does not appear to cluster into corresponding categories.\n",
    "\n",
    "##### Visualizing training sample using t-SNE\n",
    "\n",
    "[t-distributed Stochastic Neighbor Embedding (t-SNE)](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html#sklearn.manifold.TSNE) is a tool to visualize high-dimensional data. It converts similarities between data points to joint probabilities and tries to minimize the Kullback-Leibler divergence between the joint probabilities of the low-dimensional embedding and the high-dimensional data. For more details on t-SNE including other use cases see this excellent *Toward Data Science* [blog post](https://towardsdatascience.com/an-introduction-to-t-sne-with-python-example-5a3a293108d1)\n",
    "\n",
    "It is highly recommended to use another dimensionality reduction method (e.g. PCA) to reduce the number of dimensions to a reasonable amount if the number of features is very high. This will suppress some noise and speed up the computation of pairwise distances between samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import sklearn.decomposition\n",
    "import sklearn.pipeline\n",
    "import sklearn.manifold\n",
    "\n",
    "_prng = np.random.RandomState(42)\n",
    "\n",
    "embedding2_pipeline = sklearn.pipeline.make_pipeline(\n",
    "    sklearn.decomposition.PCA(n_components=0.95, random_state=_prng),\n",
    "    sklearn.manifold.TSNE(n_components=2, random_state=_prng))\n",
    "\n",
    "embedding3_pipeline = sklearn.pipeline.make_pipeline(\n",
    "    sklearn.decomposition.PCA(n_components=0.95, random_state=_prng),\n",
    "    sklearn.manifold.TSNE(n_components=3, random_state=_prng))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 10% of the data\n",
    "\n",
    "_prng = np.random.RandomState(42)\n",
    "\n",
    "_idxs = _prng.randint(y_train_flat.shape[0], size=y_train_flat.shape[0] // 10)\n",
    "tsne_features = x_train_flat[_idxs]\n",
    "tsne_category = y_train_flat[_idxs]\n",
    "print(\"tsne_features:\", tsne_features.shape, \n",
    "      \"tsne_category\", tsne_category.shape, \n",
    "      \"min,max category:\", tsne_category.min(), tsne_category.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-SNE is SLOW (but can be GPU accelerated!); \n",
    "#       lengthy operation, be prepared to wait...\n",
    "\n",
    "transform2_tsne_features = embedding2_pipeline.fit_transform(tsne_features)\n",
    "\n",
    "print(\"transform2_tsne_features:\", transform2_tsne_features.shape)\n",
    "for i in range(2):\n",
    "    print(\"min,max features[%s]:\" % i, \n",
    "          transform2_tsne_features[:,i].min(), \n",
    "          transform2_tsne_features[:,i].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_scatter_plot(transform2_tsne_features, tsne_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-SNE is SLOW (but can be GPU accelerated!); \n",
    "#       lengthy operation, be prepared to wait...\n",
    "\n",
    "transform3_tsne_features = embedding3_pipeline.fit_transform(tsne_features)\n",
    "\n",
    "print(\"transform3_tsne_features:\", transform3_tsne_features.shape)\n",
    "for i in range(3):\n",
    "    print(\"min,max features[%s]:\" % i, \n",
    "          transform3_tsne_features[:,i].min(), \n",
    "          transform3_tsne_features[:,i].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "category_scatter3d_plot(transform3_tsne_features, tsne_category)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t-SNE relates the data points (images) according to their closest neighbours.  Hints of underlying categories appear; but not cleanly seperable into the original categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Conversion\n",
    "\n",
    "The data type for the training data is `uint8`, while the input type for the network will be `float32` so the data must be converted.  Also, the data should be normalized, and the labels need to be categorical.  I.e., instead of label existing as 10 different values in a 1-D space, they need to exist as Boolean values in a 10-D space — one dimension for each category, and either a 0 or 1 value in each dimension to represent membership in that category.\n",
    "\n",
    "* https://keras.io/examples/cifar10_cnn/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = (y_train.max() - y_train.min()) + 1\n",
    "print('num_classes =', num_classes)\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "train_data = (x_train, y_train)\n",
    "test_data = (x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('x_train type:', type(x_train))\n",
    "print('x_train dtype:', x_train.dtype)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "print('y_train type:', type(y_train))\n",
    "print('y_train dtype:', y_train.dtype)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('y_test shape:', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acquire Pre-Trained Network\n",
    "\n",
    "Download an *ImageNet* pretrained VGG16 network[<sup>1</sup>](#fn1), sans classification layer, shaped for 32x32px colour images<sup>[*](https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5)</sup> (the smallest supported size).  This image-feature detection network is an example of a deep CNN (Convolutional Neural Network).\n",
    "\n",
    "**Note:** The network must be fixed – it was already trained on a very large dataset, so training it on our smaller dataset would result in it un-learning valuable generic features.\n",
    "\n",
    "<span id=\"fn1\"><sup>[1]</sup> *Very Deep Convolutional Networks for Large-Scale Image Recognition** by Karen Simonyan and Andrew Zisserman, [arXiv (2014)](https://arxiv.org/abs/1409.1556).</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "\n",
    "conv_base = VGG16(weights='imagenet', include_top=False, input_shape=(32, 32, 3))\n",
    "conv_base.trainable = False\n",
    "conv_base.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input layer shape and data type should match with the input data:\n",
    "\n",
    "*Note:* The first dimension of the shape will differ; the input layer has `None` to indicate it accepts an a batch sized collection of arrays of the remaining shape.  The data shape has a number indicating how many samples it contains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"input layer shape:\", conv_base.layers[0].input.shape)\n",
    "print(\"input layer dtype:\", conv_base.layers[0].input.dtype)\n",
    "conv_base.layers[0].input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"input data shape:\", x_train.shape)\n",
    "print(\"input data dtype:\", x_train.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore Convolutional Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cifar10_image_plot(img_data=None, image_index=None):\n",
    "    (x_imgs, y_imgs) = img_data if img_data else (x_train, y_train)\n",
    "\n",
    "    if not image_index:\n",
    "        image_index = int(random.uniform(0, x_imgs.shape[0]))\n",
    "\n",
    "    plt.imshow(x_imgs[image_index], cmap='gray')\n",
    "    plt.title(\"%s\" % cifar10_label(y_imgs[image_index]))\n",
    "    plt.xlabel(\"#%s\" % image_index)\n",
    "    plt.show()\n",
    "    \n",
    "    return image_index\n",
    "\n",
    "def get_model_layer(model, layer_name):\n",
    "    if type(layer_name) == str:\n",
    "        layer = model.get_layer(layer_name)\n",
    "    else:\n",
    "        m = model\n",
    "        for ln in layer_name:\n",
    "            model = m\n",
    "            m = m.get_layer(ln)\n",
    "        layer = m\n",
    "    return (model, layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_conv_layer_weights(model, layer_name):\n",
    "    (model, layer) = get_model_layer(model, layer_name)\n",
    "    layer_weights = layer.weights[0]\n",
    "\n",
    "    max_size = layer_weights.shape[3]\n",
    "    col_size = 12\n",
    "    row_size = int(np.ceil(float(max_size) / float(col_size)))\n",
    "\n",
    "    print(\"conv layer: %s shape: %s size: (%s,%s) count: %s\" % \n",
    "          (layer_name,\n",
    "           layer_weights.shape,\n",
    "           layer_weights.shape[0], layer_weights.shape[1],\n",
    "           max_size))\n",
    "\n",
    "    fig, ax = plt.subplots(row_size, col_size, figsize=(12, 1.2 * row_size))\n",
    "    idx = 0\n",
    "\n",
    "    for row in range(0,row_size):\n",
    "        for col in range(0,col_size):\n",
    "            ax[row][col].set_xticks([])\n",
    "            ax[row][col].set_yticks([])\n",
    "            if idx < max_size:\n",
    "                ax[row][col].imshow(layer_weights[:, :, 0, idx], cmap='gray')\n",
    "            else:\n",
    "                fig.delaxes(ax[row][col])\n",
    "            idx += 1\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_conv_layer_output(model, layer_name, image_index=None):\n",
    "    (model, layer) = get_model_layer(model, layer_name)\n",
    "    layer_output = layer.output\n",
    "\n",
    "    if not image_index:\n",
    "        image_index = cifar10_image_plot()\n",
    "        \n",
    "    intermediate_model = keras.models.Model(inputs = model.input, outputs=layer_output) \n",
    "    intermediate_prediction = intermediate_model.predict(x_train[image_index].reshape(1,32,32,3))\n",
    "  \n",
    "    max_size = layer_output.shape[3]\n",
    "    col_size = 10\n",
    "    row_size = int(np.ceil(float(max_size) / float(col_size)))\n",
    "\n",
    "    print(\"conv layer: %s shape: %s size: (%s,%s) count: %s\" % \n",
    "          (layer_name,\n",
    "           layer_output.shape,\n",
    "           layer_output.shape[1], layer_output.shape[2],\n",
    "           max_size))\n",
    "    \n",
    "    fig, ax = plt.subplots(row_size, col_size, figsize=(12, 1.2 * row_size))\n",
    "    idx = 0\n",
    "\n",
    "    for row in range(0,row_size):\n",
    "        for col in range(0,col_size):\n",
    "            ax[row][col].set_xticks([])\n",
    "            ax[row][col].set_yticks([])\n",
    "            if idx < max_size:\n",
    "                ax[row][col].imshow(intermediate_prediction[0, :, :, idx], cmap='gray')\n",
    "            else:\n",
    "                fig.delaxes(ax[row][col])\n",
    "            idx += 1\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "\n",
    "def process_image(x):\n",
    "    epsilon = 1e-5\n",
    "    # Normalizes the tensor: centers on 0, ensures that std is 0.1 Clips to [0, 1]\n",
    "    x -= x.mean()\n",
    "    x /= (x.std() + epsilon)\n",
    "    x *= 0.1\n",
    "    x += 0.5\n",
    "    x = np.clip(x, 0, 1)\n",
    "    x *= 255\n",
    "    x = np.clip(x, 0, 255).astype('uint8')\n",
    "    return x\n",
    "\n",
    "def generate_response_pattern(model, conv_layer_output, filter_index=0):\n",
    "    #step_size = 1.0\n",
    "    epsilon = 1e-5\n",
    "\n",
    "    img_tensor = tf.Variable(tf.random.uniform((1, 32, 32, 3)) * 20 + 128.0, trainable=True)\n",
    "\n",
    "    response_model = keras.models.Model([model.inputs], [conv_layer_output])\n",
    "\n",
    "    for i in range(40):\n",
    "        with tf.GradientTape() as gtape:\n",
    "            layer_output = response_model(img_tensor)\n",
    "            loss = K.mean(layer_output[0, :, :, filter_index])\n",
    "            grads = gtape.gradient(loss, img_tensor)\n",
    "            grads /= (K.sqrt(K.mean(K.square(grads))) + epsilon)\n",
    "        img_tensor = tf.Variable(tf.add(img_tensor, grads))\n",
    "\n",
    "    img = np.array(img_tensor[0])\n",
    "    return process_image(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_conv_layer_response(model, layer_name):\n",
    "    (model, layer) = get_model_layer(model, layer_name)\n",
    "    layer_output = layer.output\n",
    "    \n",
    "    max_size = layer_output.shape[3]\n",
    "    col_size = 12\n",
    "    row_size = int(np.ceil(float(max_size) / float(col_size)))\n",
    "\n",
    "    print(\"conv layer: %s shape: %s size: (%s,%s) count: %s\" % \n",
    "          (layer_name,\n",
    "           layer_output.shape,\n",
    "           layer_output.shape[1], layer_output.shape[2],\n",
    "           max_size))\n",
    "    \n",
    "    fig, ax = plt.subplots(row_size, col_size, figsize=(12, 1.2 * row_size))\n",
    "    idx = 0\n",
    "\n",
    "    for row in range(0,row_size):\n",
    "        for col in range(0,col_size):\n",
    "            ax[row][col].set_xticks([])\n",
    "            ax[row][col].set_yticks([])\n",
    "            if idx < max_size:\n",
    "                img = generate_response_pattern(model, layer_output, idx)\n",
    "                ax[row][col].imshow(img, cmap='gray')\n",
    "                ax[row][col].set_title(\"%s\" % idx)\n",
    "            else:\n",
    "                fig.delaxes(ax[row][col])\n",
    "            idx += 1\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for n in [l.name for l in conv_base.layers if isinstance(l, keras.layers.Conv2D)][:4]:\n",
    "    visualize_conv_layer_weights(conv_base, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "image_index = cifar10_image_plot()\n",
    "for n in [l.name for l in conv_base.layers if isinstance(l, keras.layers.Conv2D)][:7]:\n",
    "    visualize_conv_layer_output(conv_base, n, image_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for n in [l.name for l in conv_base.layers if isinstance(l, keras.layers.Conv2D)][:4]:\n",
    "    visualize_conv_layer_response(conv_base, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# NOTE: Visualize mid to higher level convolutional layers; \n",
    "#       lengthy operation, be prepared to wait...\n",
    "for n in [l.name for l in conv_base.layers if isinstance(l, keras.layers.Conv2D)][4:]:\n",
    "    visualize_conv_layer_response(conv_base, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Base + Classifier Model\n",
    "\n",
    "Create a simple model that has the pre-trained CNN (Convolutional Neural Network) as a base, and adds a basic classifier on top.\n",
    "\n",
    "Notice the split of total parameters (\\~15 million) between trainable (\\~0.3 million for our classifier) and non-trainable (\\~14.7 million for the pre-trained CNN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense, Activation, Dropout\n",
    "from tensorflow.keras.applications import VGG16\n",
    "\n",
    "def create_cnnbase_classifier_model(conv_base=None):\n",
    "    if not conv_base:\n",
    "        conv_base = VGG16(weights='imagenet', include_top=False, input_shape=(32, 32, 3))\n",
    "        conv_base.trainable = False\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(conv_base)\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_transfer_cnn = create_cnnbase_classifier_model(conv_base)\n",
    "model_transfer_cnn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128 #32\n",
    "epochs = 25 #100\n",
    "learning_rate = 1e-3 #1e-4\n",
    "decay = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "model_transfer_cnn.compile(loss='categorical_crossentropy',\n",
    "                           optimizer=RMSprop(learning_rate=learning_rate, decay=decay),\n",
    "                           metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model_transfer_cnn.fit(x_train, y_train,\n",
    "                                 batch_size=batch_size,\n",
    "                                 epochs=epochs,\n",
    "                                 validation_data=(x_test, y_test),\n",
    "                                 shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize accuracy and loss for training and validation.\n",
    "\n",
    "* https://keras.io/visualization/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def history_plot(history):\n",
    "    fig = plt.figure(figsize=(12,5))\n",
    "\n",
    "    plt.title('Model accuracy & loss')\n",
    "\n",
    "    # Plot training & validation accuracy values\n",
    "    ax1 = fig.add_subplot()\n",
    "    #ax1.set_ylim(0, 1.1 * max(history.history['loss']+history.history['val_loss']))\n",
    "    ax1.set_prop_cycle(color=['green', 'red'])\n",
    "    p1 = ax1.plot(history.history['loss'], label='Train Loss')\n",
    "    p2 = ax1.plot(history.history['val_loss'], label='Test Loss')\n",
    "\n",
    "    # Plot training & validation loss values\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.set_ylim(0, 1.1 * max(history.history['accuracy']+history.history['val_accuracy']))\n",
    "    ax2.set_prop_cycle(color=['blue', 'orange'])\n",
    "    p3 = ax2.plot(history.history['accuracy'], label='Train Acc')\n",
    "    p4 = ax2.plot(history.history['val_accuracy'], label='Test Acc')\n",
    "\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "\n",
    "    pz = p3 + p4 + p1 + p2\n",
    "    plt.legend(pz, [l.get_label() for l in pz], loc='center right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_plot(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score trained model.\n",
    "scores = model_transfer_cnn.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_plot(model, test_data):\n",
    "    (x_test, y_test) = test_data\n",
    "    fig = plt.figure(figsize=(16,8))\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for i in range(40):\n",
    "        plt.subplot(4, 10, i + 1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        idx = int(random.uniform(0, x_test.shape[0]))\n",
    "        result = model.predict(x_test[idx:idx+1])[0]\n",
    "        rCorrect = True if cifar10_label(y_test[idx]) == cifar10_label(result) else False\n",
    "        rSym = '✔' if rCorrect else '✘'\n",
    "        correct += 1 if rCorrect else 0\n",
    "        total += 1\n",
    "        plt.title(\"%s %s\" % (rSym, cifar10_label(result)))\n",
    "        plt.imshow(x_test[idx], cmap=plt.get_cmap('gray'))\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"% 3.2f%% correct (%s/%s)\" % (100.0 * float(correct) / float(total), correct, total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_classes_plot(model, test_data):\n",
    "    (x_test, y_test) = test_data\n",
    "    fig = plt.figure(figsize=(16,8))\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for i in range(40):\n",
    "        plt.subplot(4, 10, i + 1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        idx = int(random.uniform(0, x_test.shape[0]))\n",
    "        result = model.predict_classes(x_test[idx:idx+1])[0]\n",
    "        rCorrect = True if cifar10_label(y_test[idx]) == cifar10_label(result) else False\n",
    "        rSym = '✔' if rCorrect else '✘'\n",
    "        correct += 1 if rCorrect else 0\n",
    "        total += 1\n",
    "        plt.title(\"%s %s\" % (rSym, cifar10_label(result)))\n",
    "        plt.imshow(x_test[idx], cmap=plt.get_cmap('gray'))\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"% 3.2f%% correct (%s/%s)\" % (100.0 * float(correct) / float(total), correct, total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_proba_plot(model, test_data):\n",
    "    (x_test, y_test) = test_data\n",
    "    fig = plt.figure(figsize=(15,15))\n",
    "    \n",
    "    for i in range(10):\n",
    "        plt.subplot(10, 2, (2*i) + 1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        idx = int(random.uniform(0, x_test.shape[0]))\n",
    "        result = model.predict_proba(x_test[idx:idx+1])[0] * 100 # prob -> percent\n",
    "        plt.title(\"%s\" % cifar10_label(y_test[idx]))\n",
    "        plt.xlabel(\"#%s\" % idx)\n",
    "        plt.imshow(x_test[idx], cmap=plt.get_cmap('gray'))\n",
    "        \n",
    "        ax = plt.subplot(10, 2, (2*i) + 2)\n",
    "        plt.bar(np.arange(len(result)), result, label='%')\n",
    "        plt.xticks(range(0, len(result) + 1))\n",
    "        ax.set_xticklabels(cifar10_label_names)\n",
    "        plt.title(\"classifier probabilities\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization* by Ramprasaath Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra [arXiv (2016)](https://arxiv.org/abs/1610.02391)\n",
    "* https://jacobgil.github.io/deeplearning/class-activation-maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "\n",
    "def generate_activation_pattern(model, conv_layer_output, category_idx, image):\n",
    "    epsilon = 1e-10\n",
    "\n",
    "    activation_model = keras.models.Model([model.inputs], [conv_layer_output, model.output])\n",
    "\n",
    "    with tf.GradientTape() as gtape:\n",
    "        conv_output, prediction = activation_model(image)\n",
    "        category_output = prediction[:, category_idx]\n",
    "        grads = gtape.gradient(category_output, conv_output)\n",
    "        pooled_grads = K.mean(grads, axis=(0, 1, 2))\n",
    "\n",
    "    heatmap = tf.reduce_mean(tf.multiply(pooled_grads, conv_output), axis=-1) * -1.\n",
    "    heatmap = np.maximum(heatmap, 0)\n",
    "    heatmap /= np.max(heatmap) + epsilon\n",
    "    return(heatmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation_plot(model, layer_name, image_data, image_index=None):\n",
    "    (layer_model, conv_layer) = get_model_layer(model, layer_name)\n",
    "    (x_imgs, y_cat) = image_data\n",
    "\n",
    "    if not image_index:\n",
    "        image_index = int(random.uniform(0, x_imgs.shape[0]))\n",
    "    \n",
    "    image = x_imgs[image_index:image_index+1]\n",
    "\n",
    "    fig = plt.figure(figsize=(16,8))\n",
    "\n",
    "    plt.subplot(1, num_classes + 2, 1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.title(cifar10_label(y_cat[image_index]))\n",
    "    plt.xlabel(\"#%s\" % image_index)\n",
    "    plt.imshow(image.reshape(32, 32, 3))\n",
    "\n",
    "    result = model.predict(image)[0]\n",
    "\n",
    "    for i in range(num_classes):\n",
    "        activation = generate_activation_pattern(model, conv_layer.output, i, image)\n",
    "        activation = np.copy(activation)\n",
    "        plt.subplot(1, num_classes + 2, i + 2)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.title(cifar10_label(i))\n",
    "        plt.xlabel(\"(% 3.2f%%)\" % (result[i] * 100.0))\n",
    "        plt.imshow(activation[0])\n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prediction_classes_plot(model_transfer_cnn, (x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prediction_proba_plot(model_transfer_cnn, (x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO: Complete activation plot\n",
    "#activation_plot(model_transfer_cnn, ('vgg16', 'block5_conv3'), (x_test, y_test), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Classifier Model\n",
    "\n",
    "Create a basic CNN (Convolutional Neural Network) based classifier from scratch.\n",
    "\n",
    "Notice the total number of parameters (\\~1.25 million) in this smaller network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense, Activation, Dropout, Conv2D, MaxPooling2D\n",
    "\n",
    "def create_cnn_classifier_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), padding='same',\n",
    "                     input_shape=x_train.shape[1:]))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(32, (3, 3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(64, (3, 3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_simple_cnn = create_cnn_classifier_model()\n",
    "model_simple_cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128 #32\n",
    "epochs = 25 #100\n",
    "learning_rate = 1e-3 #1e-4\n",
    "decay = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "model_simple_cnn.compile(loss='categorical_crossentropy',\n",
    "                         optimizer=RMSprop(learning_rate=learning_rate, decay=decay),\n",
    "                         metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model_simple_cnn.fit(x_train, y_train,\n",
    "                               batch_size=batch_size,\n",
    "                               epochs=epochs,\n",
    "                               validation_data=(x_test, y_test),\n",
    "                               shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "history_plot(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score trained model.\n",
    "scores = model_simple_cnn.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prediction_classes_plot(model_simple_cnn, (x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prediction_proba_plot(model_simple_cnn, (x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for n in [l.name for l in model_simple_cnn.layers if isinstance(l, keras.layers.Conv2D)][:4]:\n",
    "    visualize_conv_layer_weights(model_simple_cnn, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "image_index = cifar10_image_plot()\n",
    "for n in [l.name for l in model_simple_cnn.layers if isinstance(l, keras.layers.Conv2D)]:\n",
    "    visualize_conv_layer_output(model_simple_cnn, n, image_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for n in [l.name for l in model_simple_cnn.layers if isinstance(l, keras.layers.Conv2D)][:4]:\n",
    "    visualize_conv_layer_response(model_simple_cnn, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n = [l.name for l in model_simple_cnn.layers if isinstance(l, keras.layers.Conv2D)][-1]\n",
    "print(n)\n",
    "for i in range(5):\n",
    "    activation_plot(model_simple_cnn, n, (x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combined Models\n",
    "\n",
    "Keras supports a functional interface to take network architectures beyond simply sequential networks.\n",
    "\n",
    "We'll demonstrate by creating a new network which combines to two CNN classifier networks we created previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Concatenate, Flatten, Dense, Activation, Dropout\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "def create_combined_classifier_model(trained_model1=None, trained_model2=None):\n",
    "    if trained_model1:\n",
    "        network1 = trained_model1\n",
    "        network1.trainable = False\n",
    "    else:\n",
    "        network1 = create_cnnbase_classifier_model()\n",
    "\n",
    "    if trained_model2:\n",
    "        network2 = trained_model2\n",
    "        network2.trainable = False\n",
    "    else:\n",
    "        network2 = create_cnn_classifier_model()\n",
    "\n",
    "    inputs = Input(shape=(32,32,3), name='cifar10_image')\n",
    "    c1 = network1(inputs)\n",
    "    c2 = network2(inputs)\n",
    "    c = Concatenate()([c1, c2])\n",
    "    x = Dense(512)(c)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(num_classes)(x)\n",
    "    outputs = Activation('softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs, name='combined_cnn_classifier')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combining Pre-Trained Models\n",
    "\n",
    "This version of the combined classifier uses both of the trained networks we created previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_combined = create_combined_classifier_model(model_transfer_cnn, model_simple_cnn)\n",
    "model_combined.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "keras.utils.plot_model(model_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128 #32\n",
    "epochs = 5 #100\n",
    "learning_rate = 1e-3 #1e-4\n",
    "decay = 1e-6\n",
    "\n",
    "model_combined.compile(loss='categorical_crossentropy',\n",
    "                         optimizer=RMSprop(learning_rate=learning_rate, decay=decay),\n",
    "                         metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model_combined.fit(x_train, y_train,\n",
    "                             batch_size=batch_size,\n",
    "                             epochs=epochs,\n",
    "                             validation_data=(x_test, y_test),\n",
    "                             shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_plot(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score trained model.\n",
    "scores = model_combined.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# NOTE: Sequential Model provides `predict_classes` or `predict_proba`\n",
    "#       Functional API Model does not; because it may have multiple outputs\n",
    "# Using simple `predict` plot instead\n",
    "prediction_plot(model_combined, (x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The combine model improves accuracy by 2%, and takes 1/5<sup>th</sup> of the time to train.\n",
    "\n",
    "#### Training Combining Models\n",
    "\n",
    "This version of the combined classifier uses both network architectures seen previously; except, in this version, the models need to be trained from scratch.  The following cells repeat the previous experiments with this combined classifier.\n",
    "\n",
    "*Spoiler:* The combined network doesn't perform any better than the partially trained one did, but takes much longer to train (more epochs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128 #32\n",
    "epochs = 25 #100\n",
    "learning_rate = 1e-3 #1e-4\n",
    "decay = 1e-6\n",
    "\n",
    "model_combined = create_combined_classifier_model()\n",
    "model_combined.compile(loss='categorical_crossentropy',\n",
    "                         optimizer=RMSprop(learning_rate=learning_rate, decay=decay),\n",
    "                         metrics=['accuracy'])\n",
    "history = model_combined.fit(x_train, y_train,\n",
    "                             batch_size=batch_size,\n",
    "                             epochs=epochs,\n",
    "                             validation_data=(x_test, y_test),\n",
    "                             shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_plot(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score trained model.\n",
    "scores = model_combined.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skip Connections\n",
    "\n",
    "From previous comparisons of the `visualize_conv_layer_response` plots of the two basic CNN models, it becomes apparent that the pre-trained VGG16 network contains more complex *knowledge* about images: there were more convolutional layers with a greater variety of patterns and features they could represent.\n",
    "\n",
    "In the previous cnnbase_classifier model `model_transfer_cnn`, only the last Conv2D layer fed directly to the classifier, and the feature information contained in the middle layers wasn't directly available to the classifier.\n",
    "\n",
    "Skip Connections are a way to bring lower level feature encodings to higher levels of the network directly.  They are also useful during training very deep networks to deal with the problem of *vanishing gradients*.\n",
    "\n",
    "In the following example, the original CNN base of the pre-trained VGG16 model is decomposed into layered groups, and a new network created that feeds these intermediate layers to the top of the network, where they are concatenated together to perform the final classification.\n",
    "\n",
    "* https://towardsdatascience.com/understanding-and-coding-a-resnet-in-keras-446d7ff84d33\n",
    "* https://arxiv.org/abs/1608.04117"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Input, Concatenate, Flatten, Dense, Activation, Dropout\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "def create_cnnbase_skipconnected_classifier_model(conv_base=None):\n",
    "    if not conv_base:\n",
    "        conv_base = VGG16(weights='imagenet', include_top=False, input_shape=(32, 32, 3))\n",
    "        conv_base.trainable = False\n",
    "\n",
    "    # Split conv_base into groups of CNN layers topped by a MaxPooling2D layer\n",
    "    cb_idxs = [i for (i,l) in enumerate(conv_base.layers) if isinstance(l, keras.layers.MaxPooling2D)]\n",
    "    all_idxs = [-1] + cb_idxs\n",
    "    idx_pairs = [l for l in zip(all_idxs, cb_idxs)]\n",
    "    cb_layers = [conv_base.layers[i+1:j+1] for (i,j) in idx_pairs]\n",
    "\n",
    "    def dense_classes(l):\n",
    "        x = Dense(512)(l)\n",
    "        x = Activation('relu')(x)\n",
    "        x = Dropout(0.5)(x)\n",
    "        x = Dense(num_classes)(x)\n",
    "        return x\n",
    "    \n",
    "    inputs = Input(shape=(32,32,3), name='cifar10_image')\n",
    "\n",
    "    # Join split groups into a sequence, but keep track of their outputs to create skip connections\n",
    "    skips = []\n",
    "    inz = inputs\n",
    "    for lz in cb_layers:\n",
    "        m = Sequential()\n",
    "        m.trainable = False\n",
    "        for ls in lz:\n",
    "            m.add(ls)\n",
    "        # inz is the output of model m, but the input for next layer group\n",
    "        inz = m(inz)\n",
    "        skips += [inz]\n",
    "\n",
    "    # Flatten all outputs (which had different dimensions) to Concatenate them on a common axis\n",
    "    flats = [dense_classes(Flatten()(l)) for l in skips]\n",
    "    c = Concatenate()(flats)\n",
    "    x = dense_classes(c)\n",
    "    outputs = Activation('softmax')(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_skipconnected = create_cnnbase_skipconnected_classifier_model(conv_base)\n",
    "model_skipconnected.summary()\n",
    "keras.utils.plot_model(model_skipconnected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128 #32\n",
    "epochs = 25 #100\n",
    "learning_rate = 1e-3 #1e-4\n",
    "decay = 1e-6\n",
    "\n",
    "model_skipconnected.compile(loss='categorical_crossentropy',\n",
    "                         optimizer=RMSprop(learning_rate=learning_rate, decay=decay),\n",
    "                         metrics=['accuracy'])\n",
    "history = model_skipconnected.fit(x_train, y_train,\n",
    "                                  batch_size=batch_size,\n",
    "                                  epochs=epochs,\n",
    "                                  validation_data=(x_test, y_test),\n",
    "                                  shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_plot(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score trained model.\n",
    "scores = model_skipconnected.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Using simple `predict` plot because model uses Functional API\n",
    "prediction_plot(model_skipconnected, (x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Agumentation\n",
    "\n",
    "Data augmentation is a technique to expand the set of available training data and can significantly improve the performance of image processing networks.\n",
    "\n",
    "**Note:** Training examples in this section may take significant time. The approach does not improve accuracy result on this simple dataset, but is included here for illustration of the technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "    samplewise_center=False,  # set each sample mean to 0\n",
    "    featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "    samplewise_std_normalization=False,  # divide each input by its std\n",
    "    zca_whitening=False,  # apply ZCA whitening\n",
    "    zca_epsilon=1e-06,  # epsilon for ZCA whitening\n",
    "    rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "    # randomly shift images horizontally (fraction of total width)\n",
    "    width_shift_range=0.1,\n",
    "    # randomly shift images vertically (fraction of total height)\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.1,  # set range for random shear\n",
    "    zoom_range=0.1,  # set range for random zoom\n",
    "    channel_shift_range=0.0,  # set range for random channel shifts\n",
    "    # set mode for filling points outside the input boundaries\n",
    "    fill_mode='nearest',\n",
    "    cval=0.0,  # value used for fill_mode = \"constant\"\n",
    "    horizontal_flip=True,  # randomly flip images\n",
    "    vertical_flip=False,  # randomly flip images\n",
    "    # set rescaling factor (applied before any other transformation)\n",
    "    rescale=None,\n",
    "    # set function that will be applied on each input\n",
    "    preprocessing_function=None,\n",
    "    # image data format, either \"channels_first\" or \"channels_last\"\n",
    "    data_format=None,\n",
    "    # fraction of images reserved for validation (strictly between 0 and 1)\n",
    "    validation_split=0.0\n",
    ")\n",
    "\n",
    "# Compute quantities required for feature-wise normalization\n",
    "# (std, mean, and principal components if ZCA whitening is applied).\n",
    "datagen.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "exampledata = datagen.flow(x_train, y_train, batch_size=batch_size)\n",
    "\n",
    "cifar10_imageset_plot((exampledata[0][0], exampledata[0][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN Base + Classifier Model Agumented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128 #32\n",
    "epochs = 12 #25 #100\n",
    "learning_rate = 1e-3 #1e-4\n",
    "decay = 1e-6\n",
    "\n",
    "model_augmented = create_cnnbase_classifier_model(conv_base)\n",
    "\n",
    "model_augmented.compile(loss='categorical_crossentropy',\n",
    "                        optimizer=RMSprop(learning_rate=learning_rate, decay=decay),\n",
    "                        metrics=['accuracy'])\n",
    "\n",
    "history = model_augmented.fit(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
    "                              validation_data=(x_test, y_test),\n",
    "                              epochs=epochs,\n",
    "                              shuffle=True,\n",
    "                              use_multiprocessing=True, workers=4\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "history_plot(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score trained model.\n",
    "scores = model_augmented.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN Classifier Model Augmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128 #32\n",
    "epochs = 12 #25 #100\n",
    "learning_rate = 1e-3 #1e-4\n",
    "decay = 1e-6\n",
    "\n",
    "model_augmented = create_cnn_classifier_model()\n",
    "\n",
    "model_augmented.compile(loss='categorical_crossentropy',\n",
    "                        optimizer=RMSprop(learning_rate=learning_rate, decay=decay),\n",
    "                        metrics=['accuracy'])\n",
    "\n",
    "history = model_augmented.fit(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
    "                              validation_data=(x_test, y_test),\n",
    "                              epochs=epochs,\n",
    "                              shuffle=True,\n",
    "                              use_multiprocessing=True, workers=4\n",
    "                             ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "history_plot(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score trained model.\n",
    "scores = model_augmented.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN Base + Skip Connected Classifier Model Agumented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128 #32\n",
    "epochs = 12 #25 #100\n",
    "learning_rate = 1e-3 #1e-4\n",
    "decay = 1e-6\n",
    "\n",
    "model_augmented = create_cnnbase_skipconnected_classifier_model(conv_base)\n",
    "\n",
    "model_augmented.compile(loss='categorical_crossentropy',\n",
    "                        optimizer=RMSprop(learning_rate=learning_rate, decay=decay),\n",
    "                        metrics=['accuracy'])\n",
    "\n",
    "history = model_augmented.fit(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
    "                              validation_data=(x_test, y_test),\n",
    "                              epochs=epochs,\n",
    "                              shuffle=True,\n",
    "                              use_multiprocessing=True, workers=4\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "history_plot(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score trained model.\n",
    "scores = model_augmented.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mixed Precision\n",
    "\n",
    "**TODO**\n",
    "\n",
    "* https://www.tensorflow.org/guide/keras/mixed_precision\n",
    "* https://developer.nvidia.com/automatic-mixed-precision\n",
    "* https://docs.nvidia.com/deeplearning/sdk/mixed-precision-training/index.html\n",
    "\n",
    "```python\n",
    "opt = tf.train.experimental.enable_mixed_precision_graph_rewrite(opt) \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-GPU Example\n",
    "\n",
    "Using multiple GPUs on a single node is a simple way to speed up deep learning.  Keras / TensorFlow support this with a small modification to code.\n",
    "\n",
    "First, determine if multiple GPUs are available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "device_count = len(physical_devices)\n",
    "print(\"GPU count:\", device_count)\n",
    "print(\"GPU devices:\", physical_devices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When scaling to `n` GPUs, there is `n *` the available GPU memory, so we can increase the batch_size by `n`.  A larger batch size means that there is more data evaluated by the batch step, which creates a more accurate and representative loss gradient – so we can take a larger corrective step by multiply the learning_rate by `n`.  Because we are learning `n *` more each epoch, we only need `1/n`<sup>th</sup> the number of training epochs.\n",
    "\n",
    "There are additional subtleties and mitigating strategies to be aware of when scaling batch sizes larger.  Some of these are discussed in [Deep Learning at scale: Accurate, Large Mini batch SGD](https://towardsdatascience.com/deep-learning-at-scale-accurate-large-mini-batch-sgd-8207d54bfe02)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Multi-GPU Example\n",
    "assert device_count >= 2, \"Two or more GPUs required to demonstrate multi-gpu functionality\"\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler, ReduceLROnPlateau\n",
    "\n",
    "batch_size = device_count * 128 #32\n",
    "epochs = 25 // device_count + 1 #100\n",
    "learning_rate = device_count * 1e-3 #1e-4\n",
    "decay = 1e-6\n",
    "\n",
    "def lr_schedule(epoch):\n",
    "    initial_lr = device_count * 1e-3\n",
    "    warmup_epochs = 5\n",
    "    warmup_lr = (epoch + 1) * initial_lr / warmup_epochs\n",
    "    return warmup_lr if epoch <= warmup_epochs else initial_lr\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(lr_schedule, verbose=1)\n",
    "lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1), cooldown=0, patience=5, min_lr=0.5e-6)\n",
    "callbacks = [lr_reducer, lr_scheduler]\n",
    "\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "with strategy.scope():\n",
    "    model_multigpu = create_cnnbase_classifier_model()\n",
    "\n",
    "    model_multigpu.compile(loss='categorical_crossentropy',\n",
    "                           optimizer=RMSprop(learning_rate=learning_rate, decay=decay, momentum=0.5),\n",
    "                           # TODO: Explore Adam without lr_scheduling\n",
    "                           #optimizer=Adam(learning_rate=learning_rate),\n",
    "                           metrics=['accuracy'])\n",
    "\n",
    "history = model_multigpu.fit(x_train, y_train,\n",
    "                             batch_size=batch_size,\n",
    "                             epochs=epochs,\n",
    "                             validation_data=(x_test, y_test),\n",
    "                             shuffle=True,\n",
    "                             callbacks=callbacks,\n",
    "                             use_multiprocessing=True, workers=4\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_plot(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score trained model.\n",
    "scores = model_multigpu.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speaker Bios\n",
    "\n",
    "Glendon Holst is a Staff Scientist in the Visualization Core Lab at KAUST (King Abdullah University of Science and Technology) specializing in HPC workflow solutions for deep learning, image processing, and scientific visualization.\n",
    "\n",
    "Mohsin Ahmed Shaikh is a Computational Scientist in the Supercomputing Core Lab at KAUST (King Abdullah University of Science and Technology) specializing in large scale HPC applications and GPGPU support for users on Ibex (cluster) and Shaheen (supercomputer).  Mohsin holds a PhD in Computational Bioengineering, and a Post Doc, from University of Canterbury, New Zealand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "* https://qz.com/1034972/the-data-that-changed-the-direction-of-ai-research-and-possibly-the-world/\n",
    "* https://www.cs.toronto.edu/~kriz/cifar.html\n",
    "* http://yann.lecun.com/exdb/mnist/index.html\n",
    "* https://towardsdatascience.com/transfer-learning-from-pre-trained-models-f2393f124751 <br/>\n",
    "  https://towardsdatascience.com/keras-transfer-learning-for-beginners-6c9b8b7143e\n",
    "  https://machinelearningmastery.com/how-to-improve-performance-with-transfer-learning-for-deep-learning-neural-networks/\n",
    "* https://towardsdatascience.com/deep-learning-at-scale-accurate-large-mini-batch-sgd-8207d54bfe02\n",
    "* https://arxiv.org/abs/1409.1556 <br/>\n",
    "  https://arxiv.org/abs/1610.02391\n",
    "* https://www.kaggle.com/c/digit-recognizer\n",
    "* https://jupyter-notebook.readthedocs.io/en/stable/\n",
    "* https://github.com/kaust-vislab/handson-ml2\n",
    "* https://keras.io/examples/cifar10_cnn/ <br/>\n",
    "  https://keras.io/examples/cifar10_resnet/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
