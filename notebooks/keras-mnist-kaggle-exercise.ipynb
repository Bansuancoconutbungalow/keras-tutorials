{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Exercise with MNIST and Keras\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This exercise complements the topics learned in the prerequisite tutorial [Deep Learning from Pre-Trained Models with Keras](keras-transfer-learning-tutorial.ipynb).\n",
    "\n",
    "This exercise aims to prepare participants for the HPC Saudi 2020 Student AI Competition.  Participants will:\n",
    "\n",
    "* Practice what they've learned from the prerequisite *Deep Learning from Pre-Trained Models with Keras* tutorial,\n",
    "* create their own CNN based image classifier for the MNIST digits dataset,\n",
    "* and finally, submit the classification results from their model to Kaggle for evaluation.\n",
    "\n",
    "Participants are expected to bring their own laptops and sign-up for free online cloud services (e.g., Google Colab, Kaggle).  They may also need to download free, open-source software prior to arriving for the workshop.\n",
    "\n",
    "* Tutorial materials are derived from:\n",
    "  * [PyTorch Tutorials](https://github.com/kaust-vislab/pytorch-tutorials) by David Pugh."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Kaggle Account\n",
    "\n",
    "#### 1. Register for an account\n",
    "\n",
    "In order to download Kaggle competition data you will first need to create a [Kaggle](https://www.kaggle.com/) account.\n",
    "\n",
    "#### 2. Create an API key\n",
    "\n",
    "Once you have registered for a Kaggle account you will need to create some [API credentials](https://github.com/Kaggle/kaggle-api#api-credentials) in order to be able to use the `kaggle` CLI to download data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download MNIST Data\n",
    "\n",
    "If you are using Binder to run this notebook, then the data is already downloaded and available.  Skip to the next step.\n",
    "\n",
    "If you are using Google Colab to run this notebook, then you will need to download the data before proceeding.\n",
    "\n",
    "#### Download MNIST from Kaggle\n",
    "\n",
    "Provide your Kaggle username and API key in the cell below and execute the code to download the Kaggle [Digit Recognizer: Learn computer vision with the famous MNIST data](https://www.kaggle.com/c/digit-recognizer) competition data. \n",
    "\n",
    "**Note: Before attempting to download the competition data you will need to login to your Kaggle account and accept the rules for this competition.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# NOTE: Replace YOUR_USERNAME and YOUR_API_KEY with actual credentials \n",
    "export KAGGLE_USERNAME=\"YOUR_USERNAME\"\n",
    "export KAGGLE_KEY=\"YOUR_API_KEY\"\n",
    "kaggle competitions download -c digit-recognizer -p ../datasets/mnist/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (Alternative) Download MNIST from GitHub\n",
    "\n",
    "If you are running this notebook using Google Colab, but did create a Kaggle account and API key, then  dowload the data from our GitHub repository by running the code in the following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import requests\n",
    "\n",
    "#RAW_URL = \"https://raw.githubusercontent.com/kaust-vislab/keras-tutorials/master/mnist/data/raw\"\n",
    "RAW_URL = \"https://github.com/holstgr-kaust/keras-tutorials/raw/master/datasets/mnist\"\n",
    "DEST_DIR = pathlib.Path('../datasets/mnist')\n",
    "\n",
    "def fetch_mnist_data():\n",
    "    DEST_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    for n in [\"mnist.npz\", \"kaggle/train.csv\", \"kaggle/test.csv\", \"kaggle/sample_submission.csv\"]:\n",
    "        path = DEST_DIR / n\n",
    "        with path.open(mode = 'wb') as f:\n",
    "            response = requests.get(RAW_URL + \"/\" + n)\n",
    "            f.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch_mnist_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (Alternative) Download MNIST with Keras\n",
    "\n",
    "If you are running this notebook using Google Colab, but did create a Kaggle account and API key, then dowload the data using the Keras load_data() API by running the code in the following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "mnist.load_data();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "### Setup\n",
    "\n",
    "Initialize the Python environment by importing and verifying the modules we will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pathlib\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`%matplotlib inline` is a magic command that makes *matplotlib* charts and plots appear was outputs in the notebook.\n",
    "\n",
    "`%matplotlib notebook` enables semi-interactive plots that can be enlarged, zoomed, and cropped while the plot is active.  One issue with this option is that new plots appear in the active plot widget, not in the cell where the data was produced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify runtime environment\n",
    "\n",
    "try:\n",
    "    # %tensorflow_version only exists in Colab.\n",
    "    %tensorflow_version 2.x\n",
    "    IS_COLAB = True\n",
    "except Exception:\n",
    "    IS_COLAB = False\n",
    "\n",
    "assert tf.__version__ >= \"2.0\", \"TensorFlow version >= 2.0 required.\"\n",
    "\n",
    "print(\"executing_eagerly:\", tf.executing_eagerly())\n",
    "\n",
    "if not tf.test.is_gpu_available():\n",
    "    print(\"No GPUs available. Expect training to be very slow.\")\n",
    "    if IS_COLAB:\n",
    "        print(\"Go to `Runtime` > `Change runtime` and select a GPU hardware accelerator.\")\n",
    "else:\n",
    "    print(\"is_built_with_cuda:\", tf.test.is_built_with_cuda())\n",
    "    print(\"is_gpu_available:\", tf.test.is_gpu_available(), tf.test.gpu_device_name())\n",
    "\n",
    "assert sys.version_info >= (3, 5), \"Python >= 3.5 required.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Pre-processing - MNIST\n",
    "\n",
    "The previously acquired MNIST dataset is the essential input needed to train an image classification model. Before using the dataset, there are several preprocessing steps required to load the data, and create the correctly sized training, validation, and testing arrays used as input to the network.\n",
    "\n",
    "The following data preparation steps are needed before they can become inputs to the network:\n",
    "\n",
    "* Cache the downloaded dataset (to use Keras `load_data()` functionality).\n",
    "* Load the dataset (MNIST is small, and fits in memory).\n",
    "    * Convert from textual CSV files into binary tensor arrays (https://www.tensorflow.org/tutorials/load_data/csv).\n",
    "    * Reshape from (784, 1) to (28, 28,1) to (32, 32, 3)\n",
    "* Verify the shape and type of the data, and understand it...\n",
    "* Convert label indices into categorical vectors.\n",
    "* Convert image data from integer to float values, and normalize.\n",
    "  * Verify converted input data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cache Data\n",
    "\n",
    "Make downloaded data available to Keras.  Provide dataset utility functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cache MNIST Datasets\n",
    "\n",
    "for n in [\"mnist.npz\", \"kaggle/train.csv\", \"kaggle/test.csv\"]:\n",
    "    #DATA_URL = \"https://github.com/holstgr-kaust/keras-tutorials/raw/master/datasets/mnist/%s\" % n\n",
    "    DATA_URL = \"file:///\" + str(pathlib.Path(\"../datasets/mnist/%s\" % n).absolute())\n",
    "    #data_file_path = tf.keras.utils.get_file(p + n, DATA_URL)\n",
    "    data_file_path = tf.keras.utils.get_file(n.replace('/','-mnist-'), DATA_URL)\n",
    "    print(\"cached file: %s\" % n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "find ~/.keras -name \"*mnist*\" -type f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_csv_dataset(file_path, **kwargs):\n",
    "    dataset = tf.data.experimental.make_csv_dataset(\n",
    "        file_path,\n",
    "        batch_size=5, # Artificially small to make examples easier to show.\n",
    "        label_name='label',\n",
    "        na_value=\"?\",\n",
    "        num_epochs=1,\n",
    "        ignore_errors=True, \n",
    "        **kwargs)\n",
    "    return dataset\n",
    "\n",
    "def pack(features, label):\n",
    "    return tf.stack(list(features.values()), axis=-1), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_batch(dataset):\n",
    "    for batch, label in dataset.take(1):\n",
    "        print(\"{:20s}: {} :: {}\".format('label', label, type(label)))\n",
    "        for key, value in batch.items():\n",
    "              print(\"{:20s}: {} :: {}\".format(key, value.numpy(), type(value)))\n",
    "\n",
    "def show_packed(packed_dataset):\n",
    "    for features, labels in packed_dataset.take(1):\n",
    "        print(features.numpy())\n",
    "        print()\n",
    "        print(labels.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_path = \"../datasets/mnist/kaggle/train.csv\"\n",
    "test_file_path = \"../datasets/mnist/kaggle/test.csv\"\n",
    "\n",
    "raw_train_data = get_csv_dataset(train_file_path)\n",
    "packed_train_data = raw_train_data.map(pack)\n",
    "train_data = packed_train_data.shuffle(500)\n",
    "\n",
    "# NOTE: unlabelled Kaggle test dataset?\n",
    "#raw_test_data = get_dataset(test_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Alternative) Load data via Keras API.  This loads data into a `numpy` array, and the test examples are labelled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO: complete example and convert numpy array into Dataset\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "# The data, split between train and test sets:\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO: Modify Explore Data examples to use Dataset, move into Explore Data section**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore Data\n",
    "\n",
    "Explore data types, shape, and value ranges.  Ensure they make sense, and you understand the data well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_batch(raw_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_packed(packed_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "packed_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('x_train type:', type(x_train), ',', 'y_train type:', type(y_train))\n",
    "print('x_train dtype:', x_train.dtype, ',', 'y_train dtype:', y_train.dtype)\n",
    "print('x_train shape:', x_train.shape, ',', 'y_train shape:', y_train.shape)\n",
    "print('x_test shape:', x_test.shape, ',', 'y_test shape:', y_test.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('x_train (min, max, mean): (%s, %s, %s)' % (x_train.min(), x_train.max(), x_train.mean()))\n",
    "print('y_train (min, max): (%s, %s)' % (y_train.min(), y_train.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def imageset_plot(img_data=None):\n",
    "    (x_imgs, y_imgs) = img_data if img_data else (x_train, y_train)\n",
    "    fig = plt.figure(figsize=(16,8))\n",
    "\n",
    "    for i in range(40):\n",
    "        plt.subplot(4, 10, i + 1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        idx = int(random.uniform(0, x_imgs.shape[0]))\n",
    "        plt.title(\"%s\" % (y_imgs[idx]))\n",
    "        plt.imshow(x_imgs[idx], cmap=plt.get_cmap('gray'))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Show array of random labelled images with matplotlib (re-run cell to see new examples)\n",
    "imageset_plot((x_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "hist, bins = np.histogram(y_train, bins = range(y_train.min(), y_train.max() + 2))\n",
    "\n",
    "fig = plt.figure(figsize=(12,5))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.hist(y_train, bins = range(y_train.min(), y_train.max() + 2))\n",
    "plt.xticks(range(y_train.min(), y_train.max() + 2))\n",
    "plt.title(\"y_train histogram\")\n",
    "plt.subplot(1,2,2)\n",
    "plt.hist(x_train.flat, bins = range(x_train.min(), x_train.max() + 2))\n",
    "plt.title(\"x_train histogram\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('y_train histogram counts:', hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data looks reasonable: there are sufficient examples for each category (y_train) and the histogram showning mostly black (0) and near-white grayscale (>250) agrees with the examples shown previously.\n",
    "\n",
    "##### Visualizing training samples using PCA\n",
    "\n",
    "[Principal Components Analysis (PCA)](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) can be used as a visualization tool to see if there are any obvious patterns in the training samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import sklearn.decomposition\n",
    "\n",
    "_prng = np.random.RandomState(42)\n",
    "\n",
    "pca = sklearn.decomposition.PCA(n_components=3, random_state=_prng)\n",
    "\n",
    "x_train_flat = x_train.reshape(*x_train.shape[:1], -1)\n",
    "y_train_flat = y_train.reshape(y_train.shape[0])\n",
    "print(\"x_train:\", x_train.shape, \"y_train\", y_train.shape)\n",
    "print(\"x_train_flat:\", x_train_flat.shape, \"y_train_flat\", y_train_flat.shape)\n",
    "pca_train_features = pca.fit_transform(x_train_flat, y_train_flat)\n",
    "print(\"pca_train_features:\", pca_train_features.shape)\n",
    "\n",
    "# Sample 10% of the PCA results\n",
    "_idxs = _prng.randint(y_train_flat.shape[0], size=y_train_flat.shape[0] // 10)\n",
    "pca_features = pca_train_features[_idxs]\n",
    "pca_category = y_train_flat[_idxs]\n",
    "print(\"pca_features:\", pca_features.shape, \n",
    "      \"pca_category\", pca_category.shape, \n",
    "      \"min,max category:\", pca_category.min(), pca_category.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def category_scatter_plot(features, category):\n",
    "    num_category = 1 + category.max() - category.min()\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(12, 10))\n",
    "    cm = plt.cm.get_cmap('tab10', num_category)\n",
    "    sc = ax.scatter(features[:,0], features[:,1], c=category, alpha=0.4, cmap=cm)\n",
    "    ax.set_xlabel(\"Component 1\")\n",
    "    ax.set_ylabel(\"Component 2\")\n",
    "    ax.set_title(\"CIFAR10 - PCA\")\n",
    "    plt.colorbar(sc)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "def category_scatter3d_plot(features, category):\n",
    "    num_category = 1 + category.max() - category.min()\n",
    "    mean_feat = np.mean(features, axis=0)\n",
    "    std_feat = np.std(features, axis=0)\n",
    "    min_range = mean_feat - std_feat\n",
    "    max_range = mean_feat + std_feat\n",
    "    \n",
    "    fig = plt.figure(figsize=(12, 10))\n",
    "    cm = plt.cm.get_cmap('tab10', num_category)\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    sc = ax.scatter(features[:,0], features[:,1], features[:,2],\n",
    "                    c=category, alpha=0.85, cmap=cm)\n",
    "    ax.set_xlabel(\"Component 1\")\n",
    "    ax.set_ylabel(\"Component 2\")\n",
    "    ax.set_zlabel(\"Component 3\")\n",
    "    ax.set_title(\"CIFAR10 - PCA\")\n",
    "    ax.set_xlim(2.0 * min_range[0], 2.0 * max_range[0])\n",
    "    ax.set_ylim(2.0 * min_range[1], 2.0 * max_range[1])\n",
    "    ax.set_zlim(2.0 * min_range[2], 2.0 * max_range[2])\n",
    "    plt.colorbar(sc)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "category_scatter_plot(pca_features, pca_category)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** 3D PCA plot works best with `%matplotlib notebook` to enable interactive rotation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_scatter3d_plot(pca_features, pca_category)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data in its original image space does not appear to cluster into corresponding categories.\n",
    "\n",
    "##### Visualizing training sample using t-SNE\n",
    "\n",
    "[t-distributed Stochastic Neighbor Embedding (t-SNE)](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html#sklearn.manifold.TSNE) is a tool to visualize high-dimensional data. It converts similarities between data points to joint probabilities and tries to minimize the Kullback-Leibler divergence between the joint probabilities of the low-dimensional embedding and the high-dimensional data. For more details on t-SNE including other use cases see this excellent *Toward Data Science* [blog post](https://towardsdatascience.com/an-introduction-to-t-sne-with-python-example-5a3a293108d1)\n",
    "\n",
    "It is highly recommended to use another dimensionality reduction method (e.g. PCA) to reduce the number of dimensions to a reasonable amount if the number of features is very high. This will suppress some noise and speed up the computation of pairwise distances between samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import sklearn.decomposition\n",
    "import sklearn.pipeline\n",
    "import sklearn.manifold\n",
    "\n",
    "_prng = np.random.RandomState(42)\n",
    "\n",
    "embedding2_pipeline = sklearn.pipeline.make_pipeline(\n",
    "    sklearn.decomposition.PCA(n_components=0.95, random_state=_prng),\n",
    "    sklearn.manifold.TSNE(n_components=2, random_state=_prng))\n",
    "\n",
    "embedding3_pipeline = sklearn.pipeline.make_pipeline(\n",
    "    sklearn.decomposition.PCA(n_components=0.95, random_state=_prng),\n",
    "    sklearn.manifold.TSNE(n_components=3, random_state=_prng))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 10% of the data\n",
    "\n",
    "_prng = np.random.RandomState(42)\n",
    "\n",
    "_idxs = _prng.randint(y_train_flat.shape[0], size=y_train_flat.shape[0] // 10)\n",
    "tsne_features = x_train_flat[_idxs]\n",
    "tsne_category = y_train_flat[_idxs]\n",
    "print(\"tsne_features:\", tsne_features.shape, \n",
    "      \"tsne_category\", tsne_category.shape, \n",
    "      \"min,max category:\", tsne_category.min(), tsne_category.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-SNE is SLOW (but can be GPU accelerated!); \n",
    "#       lengthy operation, be prepared to wait...\n",
    "\n",
    "transform2_tsne_features = embedding2_pipeline.fit_transform(tsne_features)\n",
    "\n",
    "print(\"transform2_tsne_features:\", transform2_tsne_features.shape)\n",
    "for i in range(2):\n",
    "    print(\"min,max features[%s]:\" % i, \n",
    "          transform2_tsne_features[:,i].min(), \n",
    "          transform2_tsne_features[:,i].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_scatter_plot(transform2_tsne_features, tsne_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-SNE is SLOW (but can be GPU accelerated!); \n",
    "#       lengthy operation, be prepared to wait...\n",
    "\n",
    "transform3_tsne_features = embedding3_pipeline.fit_transform(tsne_features)\n",
    "\n",
    "print(\"transform3_tsne_features:\", transform3_tsne_features.shape)\n",
    "for i in range(3):\n",
    "    print(\"min,max features[%s]:\" % i, \n",
    "          transform3_tsne_features[:,i].min(), \n",
    "          transform3_tsne_features[:,i].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "category_scatter3d_plot(transform3_tsne_features, tsne_category)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t-SNE relates the data points (images) according to their closest neighbours.  Hints of underlying categories appear; but not cleanly seperable into the original categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Conversion\n",
    "\n",
    "The data type for the training data is `uint8`, while the input type for the network will be `float32` so the data must be converted.  Also, the data should be normalized, and the labels need to be categorical.  I.e., instead of label existing as 10 different values in a 1-D space, they need to exist as Boolean values in a 10-D space — one dimension for each category, and either a 0 or 1 value in each dimension to represent membership in that category.\n",
    "\n",
    "* https://keras.io/examples/cifar10_cnn/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = (y_train.max() - y_train.min()) + 1\n",
    "print('num_classes =', num_classes)\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"shape:\", x_train.shape, x_test.shape)\n",
    "x_train = x_train.reshape(x_train.shape + (1,))\n",
    "x_test = x_test.reshape(x_test.shape + (1,))\n",
    "print(\"reshape:\", x_train.shape, x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "train_data = (x_train, y_train)\n",
    "test_data = (x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('x_train type:', type(x_train))\n",
    "print('x_train dtype:', x_train.dtype)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "print('y_train type:', type(y_train))\n",
    "print('y_train dtype:', y_train.dtype)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('y_test shape:', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize accuracy and loss for training and validation.\n",
    "\n",
    "* https://keras.io/visualization/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def history_plot(history):\n",
    "    fig = plt.figure(figsize=(12,5))\n",
    "\n",
    "    plt.title('Model accuracy & loss')\n",
    "\n",
    "    # Plot training & validation accuracy values\n",
    "    ax1 = fig.add_subplot()\n",
    "    #ax1.set_ylim(0, 1.1 * max(history.history['loss']+history.history['val_loss']))\n",
    "    ax1.set_prop_cycle(color=['green', 'red'])\n",
    "    p1 = ax1.plot(history.history['loss'], label='Train Loss')\n",
    "    p2 = ax1.plot(history.history['val_loss'], label='Test Loss')\n",
    "\n",
    "    # Plot training & validation loss values\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.set_ylim(0, 1.1 * max(history.history['accuracy']+history.history['val_accuracy']))\n",
    "    ax2.set_prop_cycle(color=['blue', 'orange'])\n",
    "    p3 = ax2.plot(history.history['accuracy'], label='Train Acc')\n",
    "    p4 = ax2.plot(history.history['val_accuracy'], label='Test Acc')\n",
    "\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "\n",
    "    pz = p3 + p4 + p1 + p2\n",
    "    plt.legend(pz, [l.get_label() for l in pz], loc='center right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_plot(model, test_data):\n",
    "    (x_test, y_test) = test_data\n",
    "    fig = plt.figure(figsize=(16,8))\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for i in range(40):\n",
    "        plt.subplot(4, 10, i + 1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        idx = int(random.uniform(0, x_test.shape[0]))\n",
    "        result = model.predict_classes(x_test[idx:idx+1])[0]\n",
    "        rCorrect = True if np.argmax(y_test[idx]) == result else False\n",
    "        rSym = '✔' if rCorrect else '✘'\n",
    "        correct += 1 if rCorrect else 0\n",
    "        total += 1\n",
    "        plt.title(\"%s %s\" % (rSym, result))\n",
    "        plt.imshow(x_test[idx][:,:,0], cmap=plt.get_cmap('gray'))\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"% 3.2f%% correct (%s/%s)\" % (100.0 * float(correct) / float(total), correct, total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_proba_plot(model, test_data):\n",
    "    (x_test, y_test) = test_data\n",
    "    fig = plt.figure(figsize=(15,15))\n",
    "    \n",
    "    for i in range(10):\n",
    "        plt.subplot(10, 2, (2*i) + 1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        idx = int(random.uniform(0, x_test.shape[0]))\n",
    "        result = model.predict_proba(x_test[idx:idx+1])[0] * 100 # prob -> percent\n",
    "        plt.title(\"%s\" % np.argmax(y_test[idx]))\n",
    "        plt.xlabel(\"#%s\" % idx)\n",
    "        plt.imshow(x_test[idx][:,:,0], cmap=plt.get_cmap('gray'))\n",
    "        \n",
    "        ax = plt.subplot(10, 2, (2*i) + 2)\n",
    "        plt.bar(np.arange(len(result)), result, label='%')\n",
    "        plt.xticks(range(0, len(result) + 1))\n",
    "        ax.set_xticklabels(range(10))\n",
    "        plt.title(\"classifier probabilities\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Your Own CNN Classifier Model\n",
    "\n",
    "Create a basic CNN (Convolutional Neural Network) based classifier from scratch.\n",
    "\n",
    "Try and create your own deep learning model to classify the MNIST data. Refer to the prerequisite tutorial and use the [CNN Classifier Model](keras-transfer-learning-tutorial.ipynb#CNN-Classifier-Model) code as a template.  Here are a few ideas to try.\n",
    "\n",
    "1. Add more convolutional layers.\n",
    "2. Add more neurons in each convolutional layer(s).\n",
    "3. Try different activation layers.\n",
    "4. Try using a different optimizer.\n",
    "5. Try tuning the hyper-parameters of your chosen optimizer.\n",
    "6. Train the model for more epochs (but don't overfit!)\n",
    "\n",
    "* https://keras.io/examples/mnist_cnn/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense, Activation, Dropout, Conv2D, MaxPooling2D\n",
    "\n",
    "def create_my_model():\n",
    "    model = Sequential()\n",
    "\n",
    "    # Add your code here...\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = create_my_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128 #32\n",
    "epochs = 12 #25\n",
    "learning_rate = 1e-3 #1e-4\n",
    "decay = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import RMSprop, Adam\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(learning_rate=learning_rate, decay=decay),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_data=(x_test, y_test),\n",
    "                    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "history_plot(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score trained model.\n",
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prediction_plot(model, (x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prediction_proba_plot(model, (x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submitting to Kaggle\n",
    "\n",
    "Submit your model's predictions to Kaggle, using your previously created Kaggle account, and then see how well your results compare to those of your peers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-train the model using the entire training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload the original numpy arrays!\n",
    "training_target, training_features = mnist_arr[:, 0], mnist_arr[:, 1:]\n",
    "\n",
    "_prng = np.random.RandomState(42)\n",
    "\n",
    "# could also add PCA or other dimension reducation step to pipeline\n",
    "preprocessing_pipeline = pipeline.make_pipeline(\n",
    "    preprocessing.MinMaxScaler(),\n",
    "    preprocessing.FunctionTransformer(lambda X: X.astype(\"float32\")),\n",
    "    preprocessing.FunctionTransformer(lambda X: torch.from_numpy(X))\n",
    ")\n",
    "_training_features_tensor = preprocessing_pipeline.fit_transform(training_features)\n",
    "_training_target_tensor = torch.from_numpy(training_target)\n",
    "\n",
    "# create the data loader\n",
    "batch_size = 128\n",
    "_training_data = data.TensorDataset(_training_features_tensor, _training_target_tensor)\n",
    "_training_data_loader = data.DataLoader(_training_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# wrap the data loader to reshape the data as needed\n",
    "reshape = lambda X, y: (X.view(-1, 1, 28, 28).to(device), y.to(device))\n",
    "wrapped_training_data_loader = WrappedDataLoader(_training_data_loader, reshape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(model_fn,\n",
    "    loss_fn,\n",
    "    wrapped_training_data_loader,\n",
    "    opt,\n",
    "    number_epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use trained model to make predictions using the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note we use transform method and NOT fit_transform!\n",
    "_testing_features = np.loadtxt(\"../datasets/mnist/kaggle/test.csv\", \n",
    "                               delimiter=',', skiprows=1, dtype=np.int64)\n",
    "scaled_testing_features = preprocessing_pipeline.transform(_testing_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model_fn(scaled_testing_features_tensor.view(-1, 1, 28, 28).to(device))\n",
    "predictions = torch.argmax(output, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visually check model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(1,1)\n",
    "_ = ax.imshow(scaled_testing_features_tensor[0].reshape((28, 28)), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reformat predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submission format for kaggle\n",
    "!head ../datasets/mnist/kaggle/sample_submission.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "if not os.path.isdir(\"../results/kaggle-submissions\"):\n",
    "    os.makedirs(\"../results/kaggle-submissions\")\n",
    "\n",
    "timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "number_predictions, = predictions.shape\n",
    "df = pd.DataFrame({\"ImageId\": range(1, number_predictions + 1), \"Label\": predictions.cpu()})\n",
    "df.to_csv(f\"../results/kaggle-submissions/submission-{timestamp}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit to Kaggle\n",
    "\n",
    "Once you have successfully submited your predictions then you can check the [Digit-Recognizer competition](https://www.kaggle.com/c/digit-recognizer) website and see how well your best model compares to your peers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "export KAGGLE_USERNAME=\"YOUR_USERNAME\"\n",
    "export KAGGLE_KEY=\"YOUR_API_KEY\"\n",
    "kaggle competitions submit digit-recognizer \\\n",
    "  -f $(ls ../results/kaggle-submissions/submission-*.csv | tail -n 1) \\\n",
    "  -m \"My first digit recognizer submission!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mentor Bios\n",
    "\n",
    "Glendon Holst is a Staff Scientist in the Visualization Core Lab at KAUST (King Abdullah University of Science and Technology) specializing in HPC workflow solutions for deep learning, image processing, and scientific visualization.\n",
    "\n",
    "Mohsin Ahmed Shaikh is a Computational Scientist in the Supercomputing Core Lab at KAUST (King Abdullah University of Science and Technology) specializing in large scale HPC applications and GPGPU support for users on Ibex (cluster) and Shaheen (supercomputer).  Mohsin holds a PhD in Computational Bioengineering, and a Post Doc, from University of Canterbury, New Zealand."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
